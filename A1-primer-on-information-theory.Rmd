# (APPENDIX) Appendix {-}

# Primer on Information Theory I

## Surprisal

```{r, include=FALSE}
library(dplyr)
library(tidyr)
library(ggplot2)

surprisal <- function(p) log(1/p, 2)

theme_sp <- function() {
  theme(
    panel.background = element_rect(
      fill = "grey90",
      color = "black",
      linewidth = 1
    ),
    panel.grid.major = element_line(color = "grey70"),
    panel.grid.minor = element_line(color = "grey80"),
  )
}

```

In the Bayesian way of thinking, the concept of **probability** gives us a way to talk about **uncertainty**.
Roughly speaking, we quantify our **beliefs** about any outcome of a random variable by assigning each outcome a weight, making sure that all weights add up to $1$.
As the notion of probability is so fundamental to statistics and probability theory, it's easy to overlook that this is not the only way of thinking about uncertainty.
A different approach that is fundamental to **information theory** is the notion of **surprisal**.

Surprisal expresses our uncertainty about an event by **quantifying how surprised we would be, should the event occur**.

We can derive a metric for surprisal from probability.
First of all, we want surprisal and probability to be inversely proportional to one another:
The **more likely** and event is to occur, the **less surprised** we will be about it.
A simple way of expressing this would be by just using the inverse of the probability $p$:

$$
\frac{1}{p}
$$

So far so good, but this is *not* the actual formula for surprisal because this metric has an unsatisfying property:
If something is **certain** to happen, our surprisal will be $1$ ($\frac{1}{p} = \frac{1}{1} = 1$).
But if it is certain to happen, we would expect our surprisal to be $0$.

There is a function that we can use to satisfy this requirement: the logarithm.
The logarithm of $1$ for any base $b$ is $0$ which is exactly the property we require.
If we simply take the logarithm of the inverse of probability, we can scale our formula for surprisal to have the required properties.
This is the actual formula for surprisal:

$$
Surprisal(x = X) = log_b(\frac{1}{P(x = X)}) = -log_b(p)
$$

Here is what the relationship between probability and surprisal looks like graphically:

```{r}
data.frame(x = seq(0, 1, 0.0001)) %>% 
  mutate(y = surprisal(x)) %>% 
  ggplot(aes(x = x, y = y)) +
  geom_line(size = 1) +
  labs(x = "p", y = "surprisal") +
  theme_sp()

```

The function is **strictly decreasing** so probability and surprisal are antiproportional to one another.
Note that surprisal is *undefined* for a probability of $0$.
This is fine because we have no need to express our surprise about an event that will never occur.

### What about the Base of the Logarithm?

You may have noticed that we have left the base $b$ of the logarithm in the formula for surprisal open.
This is because the base of the logarithm defines the **unit of information** at which we are measuring surprisal.
The most common one (and the one we use in our approaches) is base $2$.
At base $2$, we express information in **bits** (= "binary digits").
At base $10$, the unit is a **decimal digit** (or Hartley).
At base $3$, it's a **trit**.
You get the gist.

The notion of surprisal is consistent with a Bayesian vocabulary because it acknowledges the **subjectivity of belief**.
Surprisal is also called **information content**, **self-information**, or **Shannon information**.

---

Useful resources:

- [StatQuest on surprisal and entropy (video)](https://www.youtube.com/watch?v=YtebGVx-Fxw)
- [Intuitively understanding the Shannon Entropy (video)](https://www.youtube.com/watch?v=YtebGVx-Fxw)
- [Wikipedia on information content](https://en.wikipedia.org/wiki/Information_content)

---

<!-- The surprisal of combined outcomes (e.g., three coin tosses) can just be calculated by inserting the probability of the outcome of three tosses into the surprisal equation. -->
<!-- But the surprisal of three coin tosses is also just the **sum of each individual surprisal**. -->

<!-- In a coin toss, the entropy is given by the expected value of the surprisal. -->
<!-- As the expected value is just a generalization of a weighted average, this is just the probability of a surprisal happening for a heads toss plus the probability of a surprisal happening for a tails toss. -->




## Entropy

We only need to consider discrete random variables for entropy for our cases, but entropy also generalizes for the continuous case.
Entropy is given by^[You may ask yourself: Why is $H$ the formula symbol for a metric called entropy? That's because Shannon entropy is based on Boltzmann's H-theorem in statistical thermodynamics. It is contested why Boltzmann called entropy $H$, but the common conjecture is that he actually meant the greek letter Eta ($H$) which would make a lot more sense.
There have actually been typographical analyses of his handwriting to determine whether he meant Eta, not the letter $H$!]:

$$
H(X) := -\sum_{x \in X}{p(x)log(p(x))}
$$


<!-- From youtube video: -->
<!-- https://www.youtube.com/watch?v=0GCGaw0QOhA -->

<!-- How much information, on average, would we need to encode an outcome from the distribution? -->

<!-- Explain entropy as "information required to encode outcomes" -> go into different examples and generalize -->
<!-- show probability distributions alongside them for reference -->


## How do entropy and surprisal relate to one another?

Entropy is the **expected surprisal**.
This may sound odd at first.
A measure that quantifies how *surprised* I *expect* to be sounds kind of like an oxymoron.
But it *does* make sense.

First of all, with probability, we encode our **uncertainty** about outcomes of a random variable.
We are using all the information we have, but **we know that we don't have all the information**.
Thus, we know that we will be surprised about the outcome in some way because the outcome is not certain.
Entropy gives us a way to quantify how surprised we **expect** to be about the outcome.


















<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>B A Primer on Information Theory II | The Global Brain Algorithm</title>
  <meta name="description" content="<p>This is a collection of explanations on the global brain algorithm.
We explain core concepts in simple terms that help to understand the
algorithms as a whole.</p>" />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="B A Primer on Information Theory II | The Global Brain Algorithm" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a collection of explanations on the global brain algorithm.
We explain core concepts in simple terms that help to understand the
algorithms as a whole.</p>" />
  <meta name="github-repo" content="social-protocols/global-brain" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="B A Primer on Information Theory II | The Global Brain Algorithm" />
  
  <meta name="twitter:description" content="<p>This is a collection of explanations on the global brain algorithm.
We explain core concepts in simple terms that help to understand the
algorithms as a whole.</p>" />
  




  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="informtaion-theory-primer-1.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./index.html">The Global Brain Algorithm</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>About</a></li>
<li class="part"><span><b>I Introduction</b></span></li>
<li class="chapter" data-level="1" data-path="objectives-and-rationale.html"><a href="objectives-and-rationale.html"><i class="fa fa-check"></i><b>1</b> Objectives and Rationale</a>
<ul>
<li class="chapter" data-level="1.1" data-path="objectives-and-rationale.html"><a href="objectives-and-rationale.html#problem-statement"><i class="fa fa-check"></i><b>1.1</b> Problem Statement</a></li>
<li class="chapter" data-level="1.2" data-path="objectives-and-rationale.html"><a href="objectives-and-rationale.html#an-informal-argument-model"><i class="fa fa-check"></i><b>1.2</b> An Informal Argument Model</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html"><i class="fa fa-check"></i><b>2</b> Key Concepts and Assumptions</a>
<ul>
<li class="chapter" data-level="2.1" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#establishing-causality"><i class="fa fa-check"></i><b>2.1</b> Establishing Causality</a></li>
<li class="chapter" data-level="2.2" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#distributed-reasoning"><i class="fa fa-check"></i><b>2.2</b> Distributed Reasoning</a>
<ul>
<li class="chapter" data-level="" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#example-did-an-earthquake-just-happen"><i class="fa fa-check"></i>Example: Did an Earthquake Just Happen?</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#optimizing-for-information-value"><i class="fa fa-check"></i><b>2.3</b> Optimizing for Information Value</a></li>
<li class="chapter" data-level="2.4" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#reducing-cognitive-dissonance"><i class="fa fa-check"></i><b>2.4</b> Reducing Cognitive Dissonance</a></li>
<li class="chapter" data-level="2.5" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#cognitive-dissonance-as-relative-entropy"><i class="fa fa-check"></i><b>2.5</b> Cognitive Dissonance as Relative Entropy</a></li>
<li class="chapter" data-level="2.6" data-path="key-concepts-and-assumptions.html"><a href="key-concepts-and-assumptions.html#the-causal-model"><i class="fa fa-check"></i><b>2.6</b> The Causal Model</a></li>
</ul></li>
<li class="part"><span><b>II Concepts</b></span></li>
<li class="chapter" data-level="3" data-path="estimating-upvote-rates.html"><a href="estimating-upvote-rates.html"><i class="fa fa-check"></i><b>3</b> Estimating Upvote Rates</a>
<ul>
<li class="chapter" data-level="3.1" data-path="estimating-upvote-rates.html"><a href="estimating-upvote-rates.html#the-upvote-rate"><i class="fa fa-check"></i><b>3.1</b> The Upvote Rate</a>
<ul>
<li class="chapter" data-level="" data-path="estimating-upvote-rates.html"><a href="estimating-upvote-rates.html#modeling-belief-about-the-true-upvote-rate"><i class="fa fa-check"></i>Modeling Belief about the “True” Upvote Rate</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="estimating-upvote-rates.html"><a href="estimating-upvote-rates.html#estimating-upvote-rates-naive-approach"><i class="fa fa-check"></i><b>3.2</b> Estimating Upvote Rates: Naive Approach</a></li>
<li class="chapter" data-level="3.3" data-path="estimating-upvote-rates.html"><a href="estimating-upvote-rates.html#the-bayesian-average"><i class="fa fa-check"></i><b>3.3</b> The Bayesian Average</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html"><i class="fa fa-check"></i><b>4</b> Cognitive Dissonance</a>
<ul>
<li class="chapter" data-level="4.1" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#key-concepts-from-information-theory"><i class="fa fa-check"></i><b>4.1</b> Key Concepts from Information Theory</a></li>
<li class="chapter" data-level="4.2" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#surprisal-as-a-measure-of-error"><i class="fa fa-check"></i><b>4.2</b> Surprisal as a Measure of Error</a></li>
<li class="chapter" data-level="4.3" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#total-cross-entropy"><i class="fa fa-check"></i><b>4.3</b> Total Cross Entropy</a></li>
<li class="chapter" data-level="4.4" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#total-relative-entropy-cognitive-dissonance"><i class="fa fa-check"></i><b>4.4</b> Total Relative Entropy = Cognitive Dissonance</a></li>
<li class="chapter" data-level="4.5" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#detailed-example"><i class="fa fa-check"></i><b>4.5</b> Detailed Example</a></li>
<li class="chapter" data-level="4.6" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#discussion"><i class="fa fa-check"></i><b>4.6</b> Discussion</a>
<ul>
<li class="chapter" data-level="4.6.1" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#parallel-to-machine-learning"><i class="fa fa-check"></i><b>4.6.1</b> Parallel to Machine Learning</a></li>
<li class="chapter" data-level="4.6.2" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#subtle-point-1"><i class="fa fa-check"></i><b>4.6.2</b> Subtle Point 1</a></li>
<li class="chapter" data-level="4.6.3" data-path="cognitive-dissonance.html"><a href="cognitive-dissonance.html#subtle-point-1-1"><i class="fa fa-check"></i><b>4.6.3</b> Subtle Point 1</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="information-value.html"><a href="information-value.html"><i class="fa fa-check"></i><b>5</b> Information Value</a>
<ul>
<li class="chapter" data-level="5.1" data-path="information-value.html"><a href="information-value.html#the-information-value-of-a-vote"><i class="fa fa-check"></i><b>5.1</b> The Information Value of a Vote</a></li>
<li class="chapter" data-level="5.2" data-path="information-value.html"><a href="information-value.html#information-value-of-changed-votes"><i class="fa fa-check"></i><b>5.2</b> Information Value of Changed Votes</a></li>
<li class="chapter" data-level="5.3" data-path="information-value.html"><a href="information-value.html#information-value-of-new-votes"><i class="fa fa-check"></i><b>5.3</b> Information Value of New Votes</a></li>
<li class="chapter" data-level="5.4" data-path="information-value.html"><a href="information-value.html#what-does-a-vote-mean"><i class="fa fa-check"></i><b>5.4</b> What Does a Vote Mean?</a></li>
<li class="chapter" data-level="5.5" data-path="information-value.html"><a href="information-value.html#example-1-a-storm-in-madrid"><i class="fa fa-check"></i><b>5.5</b> Example 1: A Storm in Madrid</a></li>
<li class="chapter" data-level="5.6" data-path="information-value.html"><a href="information-value.html#example-2-a-typhoon-in-oslo"><i class="fa fa-check"></i><b>5.6</b> Example 2: A Typhoon in Oslo</a></li>
<li class="chapter" data-level="5.7" data-path="information-value.html"><a href="information-value.html#desired-properties-of-information-value-formula"><i class="fa fa-check"></i><b>5.7</b> Desired Properties of Information Value Formula</a></li>
<li class="chapter" data-level="5.8" data-path="information-value.html"><a href="information-value.html#upvote-only-relative-entropy"><i class="fa fa-check"></i><b>5.8</b> Upvote-Only Relative Entropy</a></li>
<li class="chapter" data-level="5.9" data-path="information-value.html"><a href="information-value.html#example-charts"><i class="fa fa-check"></i><b>5.9</b> Example Charts</a></li>
</ul></li>
<li class="appendix"><span><b>Appendix</b></span></li>
<li class="chapter" data-level="A" data-path="informtaion-theory-primer-1.html"><a href="informtaion-theory-primer-1.html"><i class="fa fa-check"></i><b>A</b> Primer on Information Theory I</a>
<ul>
<li class="chapter" data-level="A.1" data-path="informtaion-theory-primer-1.html"><a href="informtaion-theory-primer-1.html#intuitions-about-key-concepts"><i class="fa fa-check"></i><b>A.1</b> Intuitions about Key Concepts</a></li>
</ul></li>
<li class="chapter" data-level="B" data-path="a-primer-on-information-theory-ii.html"><a href="a-primer-on-information-theory-ii.html"><i class="fa fa-check"></i><b>B</b> A Primer on Information Theory II</a>
<ul>
<li class="chapter" data-level="B.1" data-path="a-primer-on-information-theory-ii.html"><a href="a-primer-on-information-theory-ii.html#surprisal"><i class="fa fa-check"></i><b>B.1</b> Surprisal</a>
<ul>
<li class="chapter" data-level="B.1.1" data-path="a-primer-on-information-theory-ii.html"><a href="a-primer-on-information-theory-ii.html#what-about-the-base-of-the-logarithm"><i class="fa fa-check"></i><b>B.1.1</b> What about the Base of the Logarithm?</a></li>
</ul></li>
<li class="chapter" data-level="B.2" data-path="a-primer-on-information-theory-ii.html"><a href="a-primer-on-information-theory-ii.html#entropy"><i class="fa fa-check"></i><b>B.2</b> Entropy</a></li>
<li class="chapter" data-level="B.3" data-path="a-primer-on-information-theory-ii.html"><a href="a-primer-on-information-theory-ii.html#surprisal-entropy-relation"><i class="fa fa-check"></i><b>B.3</b> How do Entropy and Surprisal Relate to One Another?</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The Global Brain Algorithm</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="a-primer-on-information-theory-ii" class="section level1 hasAnchor" number="7">
<h1><span class="header-section-number">B</span> A Primer on Information Theory II<a href="a-primer-on-information-theory-ii.html#a-primer-on-information-theory-ii" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="surprisal" class="section level2 hasAnchor" number="7.1">
<h2><span class="header-section-number">B.1</span> Surprisal<a href="a-primer-on-information-theory-ii.html#surprisal" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>In the Bayesian way of thinking, the concept of <strong>probability</strong> gives us a way to talk about <strong>uncertainty</strong>.
Roughly speaking, we quantify our <strong>beliefs</strong> about any outcome of a random variable by assigning each outcome a <strong>weight</strong> (the probability we assign to that event), making sure that all weights add up to <span class="math inline">\(1\)</span>.
As the notion of probability is so fundamental to statistics and probability theory, it’s easy to overlook that this is not the only way of thinking about uncertainty.
A different approach that is fundamental to <strong>information theory</strong> is the notion of <strong>surprisal</strong>.</p>
<p>Surprisal expresses our uncertainty about an event by <strong>quantifying how surprised we would be, should the event occur</strong>.</p>
<p>The notion of surprisal is consistent with a Bayesian vocabulary because it acknowledges the <strong>subjectivity of belief</strong> and we can derive it from probability.
First of all, we want surprisal and probability to be <strong>inversely proportional</strong> to one another:
The <strong>more likely</strong> and event is to occur, the <strong>less surprised</strong> we will be about it.
A naive way of expressing this would be to just take the inverse of the probability <span class="math inline">\(p\)</span>:</p>
<p><span class="math display">\[
\frac{1}{p}
\]</span></p>
<p>So far so good, but this is <em>not</em> the actual formula for surprisal because this metric has an unsatisfying property:
If something is <strong>certain</strong> to happen, i.e., the probability is our surprisal will be <span class="math inline">\(\frac{1}{p} = \frac{1}{1} = 1\)</span>.
But if it is certain to happen, we would expect our surprisal to be <span class="math inline">\(0\)</span>.</p>
<p>There is a function that we can use to scale the inverse probability, so that we can satisfy this requirement: <strong>the logarithm</strong>.
The logarithm of <span class="math inline">\(1\)</span> for any base <span class="math inline">\(b\)</span> is <span class="math inline">\(0\)</span> which is exactly the property we require.
Thus, if we simply take the logarithm of the inverse of probability, we can scale our formula to have the required properties.
This is the actual formula for surprisal:</p>
<p><span class="math display">\[
I(x = X) = log_b(\frac{1}{P(x = X)}) = -log_b(p)
\]</span></p>
<p>We use the symbol <span class="math inline">\(I\)</span> to denote surprisal because it is also commonly referred to as <em>information content</em><a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<!-- proportionality -> omit term -->
<!-- -> logarithmically inversely proportional -->
<div id="what-about-the-base-of-the-logarithm" class="section level3 hasAnchor" number="7.1.1">
<h3><span class="header-section-number">B.1.1</span> What about the Base of the Logarithm?<a href="a-primer-on-information-theory-ii.html#what-about-the-base-of-the-logarithm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>You may have noticed that we have left the base <span class="math inline">\(b\)</span> of the logarithm in the formula for surprisal open.
This is because the base of the logarithm defines the <strong>unit of information</strong> at which we are measuring surprisal.
The most common one (and the one we use in our approaches) is base <span class="math inline">\(2\)</span>.
With base <span class="math inline">\(2\)</span>, we express information in <strong>bits</strong> (= “binary digits”).
If you choose base <span class="math inline">\(10\)</span>, the unit is a <strong>decimal digit</strong> (or Hartley) and base <span class="math inline">\(3\)</span> gives you a <strong>trit</strong>.
Thus, with different values for <span class="math inline">\(b\)</span>, we can change the unit of information at which we are expressing our surprise.</p>
<p>Here is what the relationship between probability and surprisal looks like graphically with different bases <span class="math inline">\(b\)</span>:</p>
<p><img src="_main_files/figure-html/unnamed-chunk-63-1.png" width="672" /></p>
<p>The function is <strong>strictly decreasing</strong> so surprisal <strong>inversely proportional</strong> to probability.
Note that surprisal is <em>undefined</em> for a probability of <span class="math inline">\(0\)</span>.
This is fine because we have no need to express our surprise about an event that will never occur.</p>
<!-- The surprisal of combined outcomes (e.g., three coin tosses) can just be calculated by inserting the probability of the outcome of three tosses into the surprisal equation. -->
<!-- But the surprisal of three coin tosses is also just the **sum of each individual surprisal**. -->
<!-- In a coin toss, the entropy is given by the expected value of the surprisal. -->
<!-- As the expected value is just a generalization of a weighted average, this is just the probability of a surprisal happening for a heads toss plus the probability of a surprisal happening for a tails toss. -->
</div>
</div>
<div id="entropy" class="section level2 hasAnchor" number="7.2">
<h2><span class="header-section-number">B.2</span> Entropy<a href="a-primer-on-information-theory-ii.html#entropy" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- >>> 0.99 * (-log(0.99)) + 0.01 * (-log(0.01)) -->
<!-- Traceback (most recent call last): -->
<!--   File "<stdin>", line 1, in <module> -->
<!-- NameError: name 'log' is not defined -->
<!-- >>> import math -->
<!-- >>> 0.99 * (-math.log(0.99)) + 0.01 * (-math.log(0.01)) -->
<!-- 0.056001534354847345 -->
<!-- >>> 0.5 * (-math.log(0.99)) + 0.5 * (-math.log(0.01)) -->
<!-- 2.307610260920796 -->
<!-- >>> 0.5 * (-math.log(0.5)) + 0.5 * (-math.log(0.5)) -->
<!-- 0.6931471805599453 -->
<!-- >>> 0.5 * (-math.log(0.5, 2)) + 0.5 * (-math.log(0.5, 2)) -->
<!-- 1.0 -->
<!-- >>> math.log(e) -->
<!-- Traceback (most recent call last): -->
<!--   File "<stdin>", line 1, in <module> -->
<!-- NameError: name 'e' is not defined -->
<!-- >>> math.log(math.e) -->
<!-- 1.0 -->
<!-- >>> 0.5 * (-math.log(0.99)) + 0.5 * (-math.log(0.01)) -->
<!-- 2.307610260920796 -->
<!-- >>> -->
<!-- expected value formula -->
<p>Entropy is simply the <strong>expected value of surprisal</strong>.</p>
<p>We only need to consider discrete random variables for entropy for our cases, but entropy also generalizes for the continuous case.
Entropy is given by<a href="#fn2" class="footnote-ref" id="fnref2"><sup>2</sup></a>:</p>
<p><span class="math display">\[
H(X) := -\sum_{x \in X}{p(x)log(p(x))}
\]</span></p>
<!-- From youtube video: -->
<!-- https://www.youtube.com/watch?v=0GCGaw0QOhA -->
<!-- How much information, on average, would we need to encode an outcome from the distribution? -->
<!-- Explain entropy as "information required to encode outcomes" -> go into different examples and generalize -->
<!-- show probability distributions alongside them for reference -->
</div>
<div id="surprisal-entropy-relation" class="section level2 hasAnchor" number="7.3">
<h2><span class="header-section-number">B.3</span> How do Entropy and Surprisal Relate to One Another?<a href="a-primer-on-information-theory-ii.html#surprisal-entropy-relation" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Entropy is the <strong>expected surprisal</strong>.
This may sound odd at first.
A measure that quantifies how <strong><em>surprised</em></strong> I <strong><em>expect</em></strong> to be sounds like an oxymoron.
But it <em>does</em> make sense.</p>
<p>First of all, with probability, we encode our <strong>uncertainty</strong> about outcomes of a random variable.
We are using all the information we have, but <strong>we know that we don’t have all the information</strong>.
Thus, we know that we will be surprised about the outcome in some way because the outcome is not certain.
Entropy gives us a way to quantify how surprised we <strong>expect</strong> to be about the outcome.</p>
<hr />
<p><strong>Useful resources on surprisal and entropy</strong></p>
<ul>
<li><a href="https://www.youtube.com/watch?v=YtebGVx-Fxw">StatQuest on surprisal and entropy (video)</a></li>
<li><a href="https://www.youtube.com/watch?v=YtebGVx-Fxw">Intuitively understanding the Shannon Entropy (video)</a></li>
<li><a href="https://en.wikipedia.org/wiki/Information_content">Wikipedia on information content</a></li>
</ul>
<!-- remove fluff and weasel words -->
<!-- "easily" etc -->

</div>
</div>
<div class="footnotes">
<hr />
<ol start="1">
<li id="fn1"><p>Other terms for surprisal that you might come across are <strong>self-information</strong> and <strong>Shannon information</strong>.<a href="a-primer-on-information-theory-ii.html#fnref1" class="footnote-back">↩︎</a></p></li>
<li id="fn2"><p>You may ask yourself: Why is <span class="math inline">\(H\)</span> the formula symbol for a metric called entropy? That’s because Shannon entropy is based on Boltzmann’s H-theorem in statistical thermodynamics. It is contested why Boltzmann called entropy <span class="math inline">\(H\)</span>, but the common conjecture is that he actually meant the greek letter Eta (<span class="math inline">\(H\)</span>) which would make a lot more sense.
There have actually been typographical analyses of his handwriting to determine whether he meant Eta, not the letter <span class="math inline">\(H\)</span>!<a href="a-primer-on-information-theory-ii.html#fnref2" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="informtaion-theory-primer-1.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/index.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>

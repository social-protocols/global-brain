# Cognitive Dissonance and Information Gain

As described in the [Global Brain Overview](global-brain-overview.html), the goal of the global brain algorithm is to focus users' attention on posts that reduce **cognitive dissonance** -- difference of belief that exist only exist because people have been exposed to different information. When a note on a post changes the probability that users upvote the post, then there is cognitive dissonance in proportion to the number of people who voted on the post without being shown the note. Information theory lets us easily quantify this cognitive dissonance using the concept of entropy.

In information theory, information is simply the reduction of entropy. So reducing cognitive dissonance can also be seen as gaining information. So the goal of the global brain algorithm should be to direct attention to notes with the greatest potential information gain.

## Key Concepts from Information Theory

Here's a quick summary of some basic concepts of information theory, building up from simple to complex concepts.

- **surprisal**: how surprised I am when I learn that the value of X is x:

$$Suprisal(x) = -lg~P(X=x)$$

- **entropy**: how surprised I expect to be:

$$
\begin{aligned}
    H(P) &= ùîº Suprisal(X) \\
         &= ùîº -lg~P(X)  \\
         &= ‚àë_x P(X=x) √ó -lg~P(X=x) \\
\end{aligned}
$$

- **cross-entropy**: how surprised I expect Bob to be (if Bob's beliefs are $Q$ instead of $P$):

$$
\begin{aligned}
    H(P,Q) &= ùîº Suprisal_Q(X) \\
           &= ùîº -lg~Q(X) \\
           &= ‚àë_x P(X=x) √ó -lg~Q(X=x)
\end{aligned}
$$

- **relative entropy** or **KL divergence**: how much *more* surprised I expect Bob to be than me:

$$
\begin{aligned}
        Dkl(P || Q) &= H(P,Q) - H(P) \\
                    &= ‚àë_x P(X=x) √ó lg~\frac{P(X=x)}{Q(X=x)}
\end{aligned}
$$


When dealing with binary variables then these formulas can be written as:

- **entropy**: 

$$ H(p) = - p √ó lg~p - (1-p) √ó lg~(1-p) $$

- **cross-entropy**: 

$$ H(p,q) = -p √ó -lg~q - (1-p) √ó lg~(1-q) $$

- **relative entropy or KL-divergence**:

$$ Dkl(p||q) = - p √ó lg~\frac{p}{q} - (1-p) √ó lg~\frac{1-p}{1-q} $$



## Surprisal as a Measure of Error

Surprisal can be thought of, for our purposes, as a measure of the "error" of a prediction. Whenever we predict something is very unlikely to happen, and it happens, then there is a large amount of surprisal, or error. If we predicted it was likely to happen, and it happens, there is only a small amount of error.

For example, suppose we predict that the probability of a user upvoting a post is $p$. Then suppose there are actually $upvotes$ upvotes and $downvotes$ downvotes. What is the total error of our predictions?

Every time there is an upvote, surprisal is $-lg(p)$. Our estimate of the probability of a downvote is $1-p$, so whenever there is a downvote surprisal is $-lg(1-p)$.

So our total error is:

$$
    upvotes √ó -lg(p) + downvotes √ó -lg(1-p)
$$


Since $upvotes ‚âà votesTotal√óp$, and $downvotes ‚âà votesTotal√ó(1-p)$ our total error is approximately:

$$
\begin{aligned}
    & votesTotal √ó p √ó -lg(p) + votesTotal √ó (1-p) √ó -lg(1-p) \\
    & = votesTotal √ó H(p)
\end{aligned}
$$

In other words, our error is roughly entropy per vote, times the number of votes.


## Total Cross Entropy

Let's say Alice estimates the probability of an upvote to be $q$, but Bob thinks the probability of an upvote is $p$. So Bob's measure of Alice's error will different from Alice's expectation of her own error! Bob's measure of Alice's error is:

$$
\begin{aligned}
    &~~upvotes √ó -lg(q) + downvotes √ó -lg(1-q) \\
    &= votesTotal√óp √ó -lg(q) + votesTotal√ó(1-p) √ó -lg(1-q) )  \\
    &= votesTotal√ó( p√ó-lg(q) + (1-p)√ó-lg(1-q) ) \\
    &= votesTotal√óH(p,q) \\
\end{aligned}
$$

$H(p,q)$ is the cross entropy between Bob and Alices's probabilities: Bob's estimate of how surprised Alice will be on average. 

In our case, Alice represents the average uninformed user (users who haven't seen the note on a post), and Bob represents the informed user. There are $votesTotal$ "Alices", or uninformed users. Bob expects the total error of all the uninformed users to be $votesTotal√óH(p,q)$.

## Total Relative Entropy = Cognitive Dissonance

The difference between $H(p,q)$ and $H(p)$ is the **relative entropy**, also known as the Kullback‚ÄìLeibler divergence or KL-divergence. It can be thought of as "how much **more** surprised I expect Alice to be than Bob".

A very interesting property of relative entropy is that **it is never negative**. It doesn't matter whether $p$ is greater or less than $q$. Plug in different values to the formula to see.

So Bob measure of Alice's error is always greater than his measure of his own error as long as $p ‚â† q$. He always expect Alice to be more surprised than him. That is because presumably Bob thinks he knows something Alice doesn't know. If Bob thought Alices's estimate $q$ was better than his estimate $p$, Bob would change his estimate to $q$!

Relative entropy is zero if $p$ and $q$ are the same, because cross entropy $H(p,q)$ just equals entropy $H(p)$ when $p=q$ (because $H(p,p)=H(p)$).

If we take Bob's measure of Alices's total error, minus his measure of his own total error, we get the total relative entropy: how much **more** error Bob expects for Alice than he would expect if Alice knew more. 

This total error is our measure of cognitive dissonance.

$$
\begin{aligned}
cognitiveDissonance  &= votesTotal √ó H(p,q) - votesTotal √ó H(p) \\
            &= votesTotal √ó ( H(p,q) - H(p) ) \\
            &= votesTotal √ó Dkl(p || q)
\end{aligned}
$$

So whenever the global brain learns that a note has change the probability of an upvote on a post, $p$ becomes different from $q$ and therefore cognitive dissonance is created. But as users are shown the note and change their votes so that $q$ once again approaches $p$, cognitive dissonance is reduced.



## Information Gain

When users change their votes after seeing a note, $q$ approaches $p$ and relative entropy is reduced. The reduction in relative-entropy is known as the **information gain**. The information gain from moving from $q$ to $q_1$, given informed probability $p$, is

$$
\begin{aligned}
    IG_p(q, q1) &= Dkl(p || q) - Dkl(p || q_1) \\
                &= p √ó lg~(q1/q) + (1-p) √ó lg~((1-q1)/(1-q))
\end{aligned}
$$

The total information gain is:

$$
\begin{aligned}
    totalInformationGain    &= votesTotal * IG_p(q, q1) \\
                            &= votesTotal √ó ( Dkl(p || q) - Dkl(p || q_1) ) \\
                            &= cognitiveDissonance - cognitiveDissonance\_1 
\end{aligned}
$$

### Example

Suppose a post without a note is given receives 900 upvotes and 100 downvotes. But a certain note, when shown along with the post, reduces the upvote probability to 20%, without changing the vote rate.

The total cognitive dissonance is:

$$
\begin{aligned}
        cognitiveDissonance    &= votesTotal √ó Dkl(p || q) \\
                               &= votesTotal √ó ( p √ó lg~\frac{p}{q} + (1-p) √ó lg~\frac{1-p}{1-q} ) \\
                               &= 1000 √ó ( .2 √ó lg~\frac{.2}{.9} + .8 √ó lg~\frac{.8}{.1} ) \\
                               &= 1966.01 
\end{aligned}
$$

Suppose that, after showing the note to users that already upvoted the post, 70 users change their upvote to a downvote. 

The new upvote probability $q_1$ will be approximately

$$
\begin{aligned}
    q_1  &‚âà \frac{upvotes}{votesTotal} \\
       &= \frac{votesTotal√óq - 70}{votesTotal} \\
       &= \frac{1000 * .90 - 70}{1000} \\
       &= .83 
\end{aligned}
$$

The new cognitive dissonance will therefore be

$$
\begin{aligned}
    cognitiveDissonance_1 \\
    &= votesTotal √ó DKL(p, q_1)\\
    &= votesTotal √ó DKL(.2, .83 ) \\
    &= 1376.95~bits
\end{aligned}
$$

The reduction in cognitive dissonance is the total information gain:

$$
\begin{aligned}
    totalInformationGain    &= cognitiveDissonance - cognitiveDissonance_1 \\
                            &= 1966.01~bits - 1376.95~bits \\
                            &= 589.07~bits 
\end{aligned}
$$

Which can also be calculated as:

$$
\begin{aligned}
    totalInformationGain    &= votesTotal √ó IG_p(q, q1)  \\
                            &= votesTotal √ó p √ó lg~(q1/q) + (1-p) √ó lg~((1-q1)/(1-q)) \\
                            &= 1000 √ó 0.58907~bits \\
                            &= 589.07~bits 
\end{aligned}
$$

The table below shows how relative entropy falls as users change upvotes to downvotes, reaching zero when $p=q_t$ and therefore $upvotes=p√óvotesTotal=200$.

```{r}

lg = function(n) {
  return(log(n,2))
}

cross_entropy = function(p,q) {
  if (p == 0) {
    return (-lg(1-q))
  }
  if (p == 1) {
    return (-lg(q))
  }
  return (- p * lg(q) - (1-p) * lg(1-q))
  
}

dkl = function(p, q) {
  return (cross_entropy(p,q) - cross_entropy(p,p))
}

library(dplyr)
library(tidyr)
library(ggplot2)

library(knitr)

p = 0.2
q0 = 0.9
votesTotal = 1000
steps = 10
voteRate = votesTotal / steps
step = 0:steps
attentionTotal=10000
deltaAttention = attentionTotal/steps
voteChangeRate = votesTotal*(p-q0)/attentionTotal
upvotes = votesTotal*q0 + step*deltaAttention * voteChangeRate
q = upvotes/votesTotal
dissonance = votesTotal * dkl(p,q)
gain = dissonance[1:steps] - c(dissonance[2:steps],c(0))
totalDissonance = dissonance[1]

#df = data.frame(weightedImpressions=step*deltaAttention, q_t=q, upvotes=upvotes, dissonance=dissonance,informationGain=c(NA,gain))
df = data.frame(q_t=q, upvotes=upvotes,dkl=dkl(p,q), dissonance=dissonance,informationGain=c(NA,gain))

knitr::kable(df, format="markdown")

```


```{r, fig.width=6, fig.height=5}

plot = ggplot() +
  geom_line(aes(x = upvotes, y = dissonance), color = "firebrick") +
  labs(
    title = "Cognitive Dissonance Falls (Information is Gained) votes change",
    x = "Upvotes",
    y = "Cognitive Dissonance: votesTotal * Dkl(p || q_t)",
  ) 


if(p < q0) { plot = plot + scale_x_reverse() }
plot


```


Note how changing upvotes produces diminishing returns. Focusing attention on posts and notes that maximize the rate of information gain is the overall goal of the global brain algorithm. See the next document on [information-rate.html].


## Discussion

### Parallel to Machine Learning

Cross entropy is commonly used used as the cost function in many machine learning algorithms. A neural network for example takes an input with labels (e.g. images of cats and dogs) and outputs a probability (e.g. that the image is a cat). The cost function computes how far these probability estimates are from the correct labels, and the neural network is trained by minimizing the cost function.

If $yÃÇ_i$ is the machine's predicted probability for training example $i$, and $y_i$ is the correct output (1 or 0), then the total cross entropy cost is:

$$
    \sum_i H(y_i, yÃÇ) = \sum_i y_i √ó -lg(yÃÇ_i) + (1-y_i) √ó -lg(1 - yÃÇ_i)
$$


In our case, if we say that $y_i$ are users votes, and $yÃÇ_i$ is always equal to the uninformed users prediction $q$, then our cost function is identical to the cost function used when training a neural network:

$$
\begin{aligned}
    \sum_i H(y_i, q) &= \sum_i y_i √ó -lg(yÃÇ_i) + (1-y_i) √ó -lg(1 - yÃÇ_i) \\
                     &= \sum_i y_i √ó -lg(q) + (1-y_i) √ó -lg(1 - q)  \\
                     &= upvotes √ó -lg(q) + downvotes  √ó -lg(1 - q) \\
                     &‚âà votesTotal√óH(p,q) \\
\end{aligned}
$$

So both neural networks and the global brain "learn" by reducing cross entropy. The difference is that the global brain reduces entropy not by learning to make better predictions, but by in a sense teaching users to make better predictions of how a fully-informed user would vote.

### A Subtle Point

Note that cross-entropy in our case is a measure of the total surprise of uninformed users at the hypothetical future votes of informed users. That is to say, it is:

$$
\begin{aligned}
    & votesTotal√óp √ó -lg(q) + votesTotal√ó(1-p) √ó -lg(1-q) ) \\
    &‚âà hypotheticalInformedUpvotes √ó -lg(q) + hypotheticalInformedDownvotes √ó -lg(1-q) ) \\
    &= votesTotal√óH(p,q)
\end{aligned}
$$

And not

$$
\begin{aligned}
    & votesTotal√óq √ó -lg(q) + votesTotal√ó(1-q) √ó -lg(1-q) ) \\
    &‚âà actualUpvotes √ó -lg(q) + actualDownvotes √ó -lg(1-q) ) \\
    &= votesTotal√óH(q)
\end{aligned}
$$

This is potentially confusing (it caused me a great deal of confusion before I figured this out) because we are measuring the error of Alice's estimated probability $q$ with respect to hypothetical events that have have not yet actually occurred. But shouldn't we be measuring the error of the votes that have occurred?

No, because surprisal is a measure of the error of a probability estimate: how likely the event was estimated to be, vs. what event actually occurred.

The uninformed user's total surprisal about uninformed vote events is just $votesTotal*H(q)$. But this tells us nothing about error with respect to an informed users's beliefs. The informed users's total surprisal about uninformed votes $votesTotal*H(q,p)$ is potentially interesting is a measure of the distance between actual and informed belief, but is less correct because it is measuring Bob's error, not Alices's. 

$votesTotal√óH(p,q)$ makes the most sense as a measure of the tension between the current state of user's beliefs, and what that state **should** be, which is a hypothetical future where all users have changed their upvotes because of the information in the note, and thus $actualUpvotes = hypotheticalInformedUpvotes$ and $actualDownvotes = hypotheticalInformedDownvotes$.










# Vote Rate and Upvote Probability {#vote-rate-upvote-probability}

Users can add **upvotes** and **downvotes** to posts on the platform.
We denote the event of a user adding either an upvote or a downvote to a post simply as a **vote**.
For any given post, we can represent the stream of incoming votes as a Poisson process.

$$
\text{voteRate} \sim \text{Pois}(\lambda_V)
$$

$\lambda_V$ is the rate parameter of the Poisson process.
We call this rate the **vote rate** of the post.
Posts with a high vote rate receive many votes per unit of time, posts with a low vote rate receive few.

We can represent the stream of incoming *upvotes* as a Poisson process as well:

$$
\text{upvoteRate} \sim \text{Pois}(\lambda_U)
$$

We call $\lambda_U$ the **upvote rate**.
The **downvote rate** is defined analogously, but we will mostly ignore it because it can be inferred from the upvote rate and the vote rate and is thus mostly redundant.

<!--- [TODO: graphics exemplifying vote rates] --->


Another important metric is the **upvote ratio**.
Given a tally of votes at a certain point in time, we can calculate the upvote ratio at that time $t$ which is given by $upvotes:downvotes$.
We can also express it as the fraction of upvotes over all votes:

$$
\frac{upvotes}{upvotes + downvotes}
$$

You can also think about the upvote rate as an **average vote**.
If we model the upvote rate as a random variable, we might encode upvotes with a
value of $1$ and downvotes with a value of $0$.
A sample post's gathered votes at a certain point in time $t$ might look like
this:

$$
(1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0)
$$

The post has gathered a total of 12 votes of which 3 are upvotes and 9 are
downvotes.
The upvote rate (i.e., the "average" vote) for this sample is:

$$
\rho = \frac{\sum_{i=1}^n x_i}{n} = \frac{upvotes}{upvotes + downvotes} = \frac{3}{3 + 9} = 0.25
$$


## Modeling Upvote Probabilities {#modeling-belief-upvote-probability}

We assume that each post has a "true" upvote ratio that we can only estimate by collecting votes.
We model the upvote ratio as **probability of a vote being an upvote** (we will mostly call it the **upvote probability**).
The Beta distribution is a suitable model for proportions or probabilities, so we use it to express the upvote probability:

$$
\text{upvoteProbability} \sim \beta(upvotes, downvotes)
$$

```{r, include=FALSE}
beta_data <- function(alpha, beta, samples) {
  data.frame(x = seq(0, 1, len = samples)) %>%
    mutate(y = dbeta(x, alpha, beta))
}

```

The Beta distribution has two shape parameters $\alpha$ and $\beta$ which in our case are given by our upvote and downvote counts.
To provide a more intuitive understanding of how this distribution models our upvote probability, here is an example of how it develops for an example post.

Let's say we have a post with a true upvote probability of 0.24.
We initialize the distribution with a prior of 1 upvote and 1 downvote.
This is equivalent to a uniform distribution:
We assign equal probability to any outcome, i.e., to any upvote probability.

```{r, fig.width=8, fig.height=3}
beta_data(1, 1, 1000) %>%  
  data.frame() %>% 
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", linewidth = 1) +
  geom_vline(xintercept = 0.24, color = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 4)) +
  labs(
    title = "After 0 votes",
    x = "p",
    y = "Density",
  ) +
  theme_sp()

```

Now, we observe some votes coming in.
We encode upvotes with $1$ and downvotes with $0$.

<!-- (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) -->
$$
(\color{limegreen}{1, 0, 0})
$$

```{r, fig.width=8, fig.height=3}
beta_data(2, 3, 1000) %>%  
  data.frame() %>% 
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", linewidth = 1) +
  geom_vline(xintercept = 0.24, color = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 4)) +
  labs(
    title = "After 3 votes: 1 up, 2 down",
    x = "p",
    y = "Density",
  ) +
  theme_sp()

```

<!-- (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) -->
$$
(\color{grey}{1, 0, 0,} \color{limegreen}{0, 1, 0})
$$

```{r, fig.width=8, fig.height=3}
beta_data(3, 5, 1000) %>%  
  data.frame() %>% 
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", linewidth = 1) +
  geom_vline(xintercept = 0.24, color = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 4)) +
  labs(
    title = "After 6 votes: 2 up, 4 down",
    x = "p",
    y = "Density",
  ) +
  theme_sp()

```

<!-- (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) -->
$$
(\color{grey}{1, 0, 0, 0, 1, 0,} \color{limegreen}{0, 0, 0})
$$

```{r, fig.width=8, fig.height=3}
beta_data(3, 8, 1000) %>%  
  data.frame() %>% 
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", linewidth = 1) +
  geom_vline(xintercept = 0.24, color = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 4)) +
  labs(
    title = "After 9 votes: 2 up, 7 down",
    x = "p",
    y = "Density",
  ) +
  theme_sp()

```

<!-- (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) -->
$$
(\color{grey}{1, 0, 0, 0, 1, 0, 0, 0, 0,} \color{limegreen}{1, 0, 0})
$$

```{r, fig.width=8, fig.height=3}
beta_data(4, 10, 1000) %>%  
  data.frame() %>% 
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", linewidth = 1) +
  geom_vline(xintercept = 0.24, color = "red", linetype = "dashed") +
  scale_y_continuous(limits = c(0, 4)) +
  labs(
    title = "After 12 votes: 3 up, 9 down",
    x = "p",
    y = "Density",
  ) +
  theme_sp()

```

The distribution updates and the probability mass becomes more concentrated which means we become more and more certain about our beliefs about the true upvote probability.
If the post would develop further with a similar trajectory, the following
could be the outcome after 500 votes:

```{r, fig.width=8, fig.height=3}
beta_data(120, 380, 1000) %>%  
  data.frame() %>% 
  ggplot() +
  geom_line(aes(x = x, y = y), color = "black", linewidth = 1) +
  geom_vline(xintercept = 0.24, color = "red", linetype = "dashed") +
  labs(
    title = "After 500 votes: 120 up, 380 down",
    x = "p",
    y = "Density",
  ) +
  theme_sp()

```

Our probability mass is now pretty concentrated around what (in this case) we know is the true upvote probability.


## Tallies

There are two essential forms of content users can assign votes to:
posts and posts with a note.
For all posts and post/note combinations, we keep a tally of upvotes and downvotes.

```{r}
knitr::include_graphics("images/voting-tallies.png")

```

## The Upvote Probability

We assume that each post has a "true" upvote probability: a parameter $p$ which quantifies how likely users are to give the post an upvote if they vote.
Now that we can express our beliefs about the upvote probability, how do we make a "best guess" at any given point in time?
A naive way of estimating the upvote probability is to take the actual current ratio $upvotes:downvotes$, or rather the fraction, which is equivalent to the **plain sample average vote**:

$$
P_t(upvote) = \frac{\sum_{i=1}^n x_i}{n} = \frac{upvotes}{upvotes + downvotes}
$$

This solution is naive because it ignores an important fact:
In the beginning, we have no information about the upvote probability of the post.
Thus, each new arriving vote has an outsized effect on our estimate that just
gets increasingly small over time.
This would result in very erratic estimates in the beginning which would only
smooth out over time.
Here is how the cumulative mean develops for a random vote history with a true
upvote rate of 0.6:

```{r, fig.width=8, fig.height=5}
set.seed(5)
n_votes <- 100
data <- data.frame(
  idx = seq(1, n_votes),
  x = rbinom(n_votes, size = 1, 0.6)
) %>% 
  mutate(cum_mean = cummean(x))

data %>% 
  ggplot(aes(x = idx, y = cum_mean)) +
  geom_line(color = "firebrick", linewidth = 1) +
  scale_y_continuous(limits = c(0, 1)) +
  geom_hline(yintercept = 0.6, color = "black", linetype = "dashed") +
  geom_text(x = 90, y = 0.65, label = "true upvote rate: 0.6") +
  labs(
    x = "After N votes",
    y = "Cumulative Mean"
  ) +
  theme_sp()

```

If we were to use this metric to compare posts, getting a very high upvote rate
estimate in the early stages of a post would essentially come down to luck.
It might then temporarily fare overly well (or overly poorly) compared to other
posts.
We have to take into account **prior information** to avoid this.


## The Bayesian Average

The Bayesian Average uses a **weighted prior estimate of the average** to avoid the
erratic shifts in the estimate when there is not a lot of data.
It is calculated as

$$
\frac{\color{blue}{C \cdot m} + \sum_{i=1}^n x_i}{\color{blue}{C} + n}
$$

where $C$ is a weight constant and $m$ is our prior belief about the average.
If you compare it to the plain average, <span style="color: blue">we simply add $C \cdot m$ to the nominator and $C$ to the denominator</span>.

**But what does this achieve in concrete terms?**

Let's build up to this formula step by step for estimating the
true upvote rate of a post.
First, remember that the sample upvote rate can be thought of as the "plain" average vote.
Let's say our dataset $X$ consists of $1$s and $0$s, where $1$ denotes an upvote and $0$ denotes a downvote.
Then $\sum_{i=1}^n{x_i}$ is the *number of upvotes* (because downvotes are
encoded as $0$s) and $n$ is $upvotes + downvotes$, the *total number of votes*.
Substituting the values in the Bayesian average formula gives us:

$$
\frac{C \cdot m + \sum_{i=1}^n x_i}{C + n} = \frac{C \cdot m + upvotes}{C + upvotes + downvotes}
$$

**Practically, adding these terms to our formula means that we calculate the
cumulative average as if we had collected $C$ votes with an upvote rate of $m$
before we collected the first vote on our post.**

If our prior belief about the average is 0.68 and we chose a weighting factor of
100, this would mean that we calculate the average as if we had previously
collected 100 data points which amounted to an upvote rate of exactly 0.68.

Plugging in the values makes this apparent:

$$
\frac{100 \cdot 0.68 + upvotes}{100 + upvotes + downvotes} = \frac{68 + upvotes}{100 + upvotes + downvotes}
$$

Graphically, it looks like this (the light grey line is the plain average for
comparison):

```{r, fig.width=8, fig.height=5}
set.seed(12)
n_votes <- 100
votes <- rbinom(n_votes, size = 1, 0.6)
data_plain <- data.frame(
  idx = seq(1, n_votes),
  x = votes,
  is_prior = FALSE
) %>% 
  mutate(cum_mean_plain = cummean(x)) %>% 
  union(
    data.frame(
      idx = seq(-9, 0),
      x = NA,
      is_prior = TRUE,
      cum_mean_plain = NA
    )
  ) %>% 
  arrange(idx) %>% 
  select(-x)

data_bayes <- data.frame(
  idx = seq(1, n_votes),
  x = votes,
  is_prior = FALSE
) %>% 
  union(
    data.frame(
      idx = seq(-9, 0),
      x = 0.68,
      is_prior = TRUE
    )
  ) %>% 
  arrange(idx) %>% 
  mutate(cum_mean_bayes = cummean(x)) %>% 
  select(-x)

data_plain %>% 
  left_join(data_bayes, by = c("idx", "is_prior")) %>% 
  pivot_longer(
    cols = c(cum_mean_plain, cum_mean_bayes),
    names_to = "metric",
    values_to = "value",
    values_drop_na = TRUE
  ) %>% 
  ggplot(aes(x = idx, y = value)) +
  geom_line(aes(alpha = metric), linewidth = 1) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_alpha_manual(
    values = c(
      "cum_mean_bayes" = 1.0,
      "cum_mean_plain" = 0.2
    ),
  ) +
  geom_text(x = 16, y = 0.95, label = "start of data collection") +
  geom_text(x = 90, y = 0.55, label = "true upvote rate", color = "red") +
  geom_text(x = -5, y = 0.72, label = "prior", color = "black") +
  geom_hline(yintercept = 0.6, color = "red", linetype = "dashed") +
  geom_vline(xintercept = 0, color = "black", linetype = "dashed") +
  labs(
    x = "After N votes",
    y = "Cumulative Bayesian Average"
  ) +
  theme_sp() +
  theme(legend.position = "None")

```

The cumulative Bayesian average is much less erratic when little data is available.
However, there is an important question left:
How do we chose a good prior and a good weight?

Here is how the Bayesian average develops for different prior beliefs about the
average.
Bayesian averages over time are indicated by light grey lines, the priors chosen
here are 0.1 through 0.9 and the weight is kept constant at 20.
The plain average is overlayed in darker grey for reference.

```{r, fig.width=8, fig.height=5}
C <- 20

data_plain <- data.frame(
  idx = seq(1, n_votes),
  x = votes
) %>% 
  mutate(cum_mean_plain = cummean(x))
  
data_bayes <-
  data_plain %>%
  mutate(
    bayesian_avg_10 = (C * 0.1 + cumsum(x)) / (C + idx),
    bayesian_avg_20 = (C * 0.2 + cumsum(x)) / (C + idx),
    bayesian_avg_30 = (C * 0.3 + cumsum(x)) / (C + idx),
    bayesian_avg_40 = (C * 0.4 + cumsum(x)) / (C + idx),
    bayesian_avg_50 = (C * 0.5 + cumsum(x)) / (C + idx),
    bayesian_avg_60 = (C * 0.6 + cumsum(x)) / (C + idx),
    bayesian_avg_70 = (C * 0.7 + cumsum(x)) / (C + idx),
    bayesian_avg_80 = (C * 0.8 + cumsum(x)) / (C + idx),
    bayesian_avg_90 = (C * 0.9 + cumsum(x)) / (C + idx),
  ) %>% 
  pivot_longer(
    cols = c(
      cum_mean_plain,
      bayesian_avg_10,
      bayesian_avg_20,
      bayesian_avg_30,
      bayesian_avg_40,
      bayesian_avg_50,
      bayesian_avg_60,
      bayesian_avg_70,
      bayesian_avg_80,
      bayesian_avg_90
    ),
    names_to = "metric",
    values_to = "value"
  ) %>% 
  mutate(is_bayesian_avg = stringr::str_starts(metric, "bayes"))

data_bayes %>% 
  ggplot(
    aes(
      x = idx,
      y = value,
      group = metric,
      color = metric
    )
  ) +
  geom_line(linewidth = 1) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(
    values = c(
      "bayesian_avg_10" = "grey80",
      "bayesian_avg_20" = "grey80",
      "bayesian_avg_30" = "grey80",
      "bayesian_avg_40" = "grey80",
      "bayesian_avg_50" = "grey80",
      "bayesian_avg_60" = "grey80",
      "bayesian_avg_70" = "grey80",
      "bayesian_avg_80" = "grey80",
      "bayesian_avg_90" = "grey80",
      "cum_mean" = "black"
    )
  ) +
  geom_hline(yintercept = 0.6, color = "red", linetype = "dashed") +
  geom_text(x = 90, y = 0.57, label = "true upvote rate = 0.6", color = "red") +
  theme_sp() +
  theme(legend.position = "None")

```

And here is that same plot with the same priors, but with a weight of 100.

```{r, fig.width=8, fig.height=5}
C <- 100

data_plain <- data.frame(
  idx = seq(1, n_votes),
  x = votes
) %>% 
  mutate(cum_mean_plain = cummean(x))
  
data_bayes <-
  data_plain %>%
  mutate(
    bayesian_avg_10 = (C * 0.1 + cumsum(x)) / (C + idx),
    bayesian_avg_20 = (C * 0.2 + cumsum(x)) / (C + idx),
    bayesian_avg_30 = (C * 0.3 + cumsum(x)) / (C + idx),
    bayesian_avg_40 = (C * 0.4 + cumsum(x)) / (C + idx),
    bayesian_avg_50 = (C * 0.5 + cumsum(x)) / (C + idx),
    bayesian_avg_60 = (C * 0.6 + cumsum(x)) / (C + idx),
    bayesian_avg_70 = (C * 0.7 + cumsum(x)) / (C + idx),
    bayesian_avg_80 = (C * 0.8 + cumsum(x)) / (C + idx),
    bayesian_avg_90 = (C * 0.9 + cumsum(x)) / (C + idx),
  ) %>% 
  pivot_longer(
    cols = c(
      cum_mean_plain,
      bayesian_avg_10,
      bayesian_avg_20,
      bayesian_avg_30,
      bayesian_avg_40,
      bayesian_avg_50,
      bayesian_avg_60,
      bayesian_avg_70,
      bayesian_avg_80,
      bayesian_avg_90
    ),
    names_to = "metric",
    values_to = "value"
  ) %>% 
  mutate(is_bayesian_avg = stringr::str_starts(metric, "bayes"))

data_bayes %>% 
  ggplot(
    aes(
      x = idx,
      y = value,
      group = metric,
      color = metric
    )
  ) +
  geom_line(linewidth = 1) +
  scale_y_continuous(limits = c(0, 1)) +
  scale_color_manual(
    values = c(
      "bayesian_avg_10" = "grey80",
      "bayesian_avg_20" = "grey80",
      "bayesian_avg_30" = "grey80",
      "bayesian_avg_40" = "grey80",
      "bayesian_avg_50" = "grey80",
      "bayesian_avg_60" = "grey80",
      "bayesian_avg_70" = "grey80",
      "bayesian_avg_80" = "grey80",
      "bayesian_avg_90" = "grey80",
      "cum_mean" = "black"
    )
  ) +
  geom_hline(yintercept = 0.6, color = "red", linetype = "dashed") +
  geom_text(x = 90, y = 0.57, label = "true upvote rate = 0.6", color = "red") +
  theme_sp() +
  theme(legend.position = "None")

```

<!--- TODO: write a section about how to chose a prior --->

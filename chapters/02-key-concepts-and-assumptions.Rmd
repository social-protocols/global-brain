# Key Concepts and Assumptions

## Establishing Causality

The basic computational unit in the Global Brain is the **informed vote**. Before users vote on a post, the UI shows them a **note** and then records their vote, **given** they were shown that note before voting. If the informed vote is different from the uninformed vote, it means the note **caused** changes in votes. We call notes that do change behavior **informative**. 

Every reply to every post becomes effectively an AB test that tells us how exposure to a note affects votes on a post. What makes properly-run AB tests very powerful is that they establish **causality**. They tell us not just that upvotes on the note and post are correlated, but that the note actually *causes* changes to votes on the post.

Establishing these causal links allows us to model the Global Brain as a causal Bayesian network. The causal links between notes and posts are the synapses of the Global Brain. They allow us to predict how the average user **would** vote if they were exposed to some combination of notes and posts, by predicting how exposure to those posts will influence upvotes on other posts, and how that influences upvotes on other posts, and so on. 

## Distributed Reasoning

By simulating how beliefs propagate through the network, the Global Brain engages in a form of **reasoning**, loosely defined. Each neuron in the Global Brain reasons -- or processes information -- using the same inferences the average user makes. But unlike the average user, the Global Brain can process **all** information in the system in a valid Bayesian manner. So in a way, it simulates an average user who has unlimited time, memory, and processing capacity.

We can then query this model to estimate how the Global Brain would vote if it was exposed to **all** information -- all posts ever made. We can also use the model to identify where to direct users' attention in order to reduce cognitive dissonance in the network.

### Example: Did an Earthquake Just Happen? {-}

Suppose there is a post A with a video showing the aftermath of a recent earthquake. This post goes viral, but then somebody posts a note B on the video which says "This video is actually from an earthquake in 2004. Here is a link to the original video.".

Suppose that people who are shown note B along with the video in post A are less likely to upvote A. This will often be the case, as many people probably upvoted out of simple ignorance, not malicious intent to spread misinformation. The Global Brain will conclude that:

1. B is **informative**.
2. A should receive less attention, because informed users are unlikely to upvote it.
3. B should receive more attention from users who have already seen or upvoted A in order to reduce cognitive dissonance.

Now, suppose that it turns out the information in note B is actually wrong. Somebody may reply to B with a subnote C saying "No, the video in that link is completely different! This video is recent footage from CNN. Here's a link."

The Global Brain can actually look at how C affects users' votes on B to estimate how a fully informed user **would** have voted on A. Even if there is no fully informed user who saw post C and then changed their vote on A, we can in a sense simulate a fully-informed user. This is what allows the Global Brain to act like a kind of neural network, with the links between posts acting as synapses. No matter how many posts there are in the system, we can simulate the actions of a user who has read every post ever posted in the network -- in other words, a user who has processed all available information.

For example, if post B links different groups of users in different conversations -- for example if B is "reposted" or cross-posted to more than one forum -- then the Global Brain can observe how C affects B in one forum, and how B affects A in another forum, and "link" the two forums, predicting the probability that a fully-informed user would upvote A after seeing subnote C, even if no user who voted on A actually saw subnote C.

<!-- emphasize locality -> cutting the reply tree in two pieces, treat them independently -> still works -->
<!-- works on ACYCLIC graphs -> we should avoid cyclic graphs (think through whether that could happen with cross-posting) -->

## Optimizing for Information Value

**A key assumption we make is that more information leads to better judgments.**

Even though people can post false and misleading information, people can respond by explaining why this information is false or misleading. Even though people can promote hateful or harmful ideas, people can respond with reasons why these ideas are wrong.

By identifying information that changes minds, driving attention to that information, and then identifying responses that change minds back, the system drives informative conversations. Although this isn't guaranteed to lead to the truth, it will at least result in more informed users. And it will also help identify the most informed opinions: what posts would still be upvoted by users who were exposed to all the strongest information on arguments on all sides.

In the example above, once the model learns that B is changing people's votes on A, it directs more attention to B, resulting in fewer upvotes for A. We can think of B as an argument against A, or a reason not to upvote A. The "reason" doesn't need to take any particular logical or rhetorical form. It is a reason by virtue of being **the literal reason** for -- the observed cause of -- changes to votes.

Unfortunately, in the example above, B was not true. But fortunately, C is a convincing reason to believe that B is not true. C actually causes people **not** to change their vote on A, in spite of being exposed to B. B and C cancel each other out, so to speak. So the Global Brain concludes that it is no longer helpful to direct attention to B, and that it should continue to direct attention to post A.

## Reducing Cognitive Dissonance

The Global Brain only requires a small number of people to actually change their minds in order to learn how beliefs affect other beliefs and construct the causal Bayesian network. So the predictions of the model may differ significantly from the beliefs of the majority.

Suppose 1000 users voted on a post and most votes were upvotes. Then suppose that among a subset of 30 users who saw some very informative note, the probability of upvoting the post dropped to close to zero. We can thus estimate that if all 1000 users saw that note, they would not have upvoted the post. Even though in actuality most users did upvote it.

In such situations, we say there is a large amount of **cognitive dissonance** in the network. Cognitive dissonance arises whenever people's upvotes differ from what their upvotes **would** be if they were exposed to all the relevant information. Or in other words, when people's votes differ from the predictions of the causal model.

The goal of the Global Brain algorithm is minimizing cognitive dissonance. It does this by exposing users to the informative note, reducing the inconsistency between what people upvote and what they would upvote if they were more informed.

Reducing cognitive dissonance brings individual participants into greater alignment, reducing differences of opinion that are due to differences in information. It cannot of course *eliminate* differences of opinion. Opinion can be subjective, because people have different priors and different ways of processing information. But it can identify situations where differences are due to ignorance of readily available information: when one group of people believe something that another group doesn't *only* because they haven't seen the same posts.

The result is a network that learns. The model learns as new posts are submitted to the network. And the human participants in the network also learn as they are exposed to posts that bring their mental models into greater alignment.

## Cognitive Dissonance as Relative Entropy

**We can measure cognitive dissonance as the relative entropy (or KL divergence) between users' actual beliefs and their hypothetical fully-informed beliefs (the beliefs they would have if exposed to all posts).**

Relative entropy is a way of measuring the distance, or error, between two probability distributions. It is similar to the measure used as the loss function when training a neural network, and in this sense, the Global Brain can be seen to "learn" in a sense similar to how a neural network learns.

Optimizing for reducing cross-entropy is sufficient to drive productive conversations, because it directs attention exactly where it needs to be directed. Consider again the example above, where subnote C "cancels out" the effect of note B on post A. This means that there is no difference between users beliefs about post A and their fully-informed beliefs, given they have seen B and C. So relative entropy is zero, and there is no value to be gained from directing more users' attention to note B.

On the other hand, since some users have already seen B and had their minds changed, there is a difference between their probability of voting on post A and the fully-informed probability. So there is still cognitive dissonance among the subset of users who were exposed to B. Exposing that subset of users to post C, until the upvote percentage of that subset matches the overall upvote percentage, will result in minimal cross-entropy.

The theory and math for calculating cross-entropy and ranking posts based on their potential to reduce cross-entropy is developed in [Cognitive Dissonance](#cognitive-dissonance).

## The Causal Model

The Global Brain requires modeling users' beliefs as a **causal Bayesian network**. But making this link is tricky, because we don't actually know what users believe, only how they vote. Suppose we know how much exposure to post B changed the probability that users will upvote post A, and how much exposure to post C changes the probability that a user will upvote post B. Can we predict how much exposure to post C will change the probability that a user will upvote post A?

We can draw a causal graph with our assumptions of how information **causes** changes in votes. We assume that votes are influenced by hidden variables, which are users' actual beliefs. For example, when somebody posts B, and it influences their vote on A, B must contain some information that users didn't already have, and that gives them a reason to change their vote. If B did not contain new information, why would it change their behavior?

Even a humorous comment can be modeled as information for our purposes. Suppose somebody responds to post A with a purely humorous comment B and this causes more people to upvote A. Why would a joke cause more people to upvote A? Since we assume votes indicate intent to direct attention, it can only mean that the fact that joke B was funny caused people to believe that A should get more attention. Maybe it convinced them that A could be an amusing topic.

But we don't need to have a theory of how people reason or what it is in people's minds that cause them to change their behavior. It is sufficient to assert that there is some latent variable that links exposure to a post to voting behaviors. 

To understand this process, it helps to think in purely Bayesian terms. We can say that if something causes people to change votes, it is a **reason** for the change of votes and that these reasons are unobserved "underlying beliefs" in some proposition.

Let's say ğ‘ represents users' underlying belief that was directly changed by the post B (e.g. B is a video of Bigfoot, which causes users to change their belief in ğ‘=*there is video evidence that Bigfoot exists*). The belief in ğ‘ has a causal effect on votes on B. ğ‘ also has a causal effect on some other underlying belief ğ‘Œ (e.g. ğ‘Œ=*Bigfoot exists*). Finally, the underlying belief ğ‘Œ has a causal effect on upvotes on post A ("Bigfoot is real, people!")

Then let's use the term ğ‘†ğ‘ and ğ‘‰ to represent the event that users were exposed to posts B and A respectively. And finally we'll use italic ğµ and ğ´ to represent upvotes on B and A respectively.

So showing a user post B (ğ‘†ğ‘) affects belief in ğ‘ which affects votes ğµ, and so on. And belief ğ‘ also affects belief ğ‘Œ. So our causal graph looks like this.

    ğ‘†ğ‘ â†’ ğ‘ â†’ ğµ
        â†“
        ğ‘Œ â†’ ğ´

Now, let's assume that the number of upvotes ğ´ or ğµ is proportional to the number of people that believe ğ‘Œ or ğ‘ respectively.

$$
P(A = 1) \propto P(Y = 1)
$$

$$
P(B = 1) \propto P(Z = 1)
$$

So we can kind of use ğ´ and ğµ as proxies for ğ‘Œ and ğ‘.

Now, suppose a user responds to post B with post C *That video was generated by AI!*. **Seeing** post C (ğ‘†ğ‘) indirectly causes a change in the probability that users upvote A. And for the moment, assume that ğ‘†ğ‘ *only* affects ğ‘Œ through ğ‘. So seeing post C affects the belief that ğ‘=*there is video evidence that bigfoot exists* which affects the belief ğ‘Œ=*bigfoot exists* which affects ğ´.

Our causal graph looks like this:

         ğ‘†ğ‘ â†’ ğ¶
         â†“
    ğ‘†ğ‘ â†’ ğ‘ â†’ ğµ
         â†“
         ğ‘Œ â†’ ğ´

Now, what can we do with this causal graph? Well, assuming the causal assumptions in the graph are true, we can deduce how ğ‘†ğ‘ will affect ğ‘Œ (through ğ‘), without directly observing either ğ‘Œ or ğ‘ (the hidden states in users' minds), simply by observing how ğ‘†ğ‘ affects ğµ and how ğ‘†ğ‘ affects ğ´. 

This means that we can model long chains of "reasoning" and predict how one piece of information would propagate through the Global Brain, even if there is no individual who actually engages in that chain of reasoning. For example, post B (the video of bigfoot) might be cross-posted to a forum dedicated to identifying AI-generated videos. If comments in that forum convince users in that forum that the video was AI generated, then the Global Brain can deduce that some people in the cryptobiology forum would also be convinced. The groups of users in the two forum don't have to overlap for information to flow between forums.

This means that for very complex issues, where no individual can come close to being able to process all the information in the network, the Global Brain can essentially simulate a fully-informed individual by breaking the problem down into parts and propagating information between conversations using posts that are "cross-posted".

<!-- The math for calculating belief propagation is described [in another document]. --> 


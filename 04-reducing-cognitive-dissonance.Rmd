# Reducing Cognitive Dissonance

As described in the [Global Brain Overview](global-brain-overview.html), the goal of the global brain algorithm is to focus users' attention on posts that reduce **cognitive dissonance** -- difference of belief that only exists because people have been exposed to different information. When a note on a post changes the upvote rate of the post, then there is cognitive dissonance in proportion to the number of people who voted on the post without being shown the note. Information theory lets us easily quantify this cognitive dissonance using the concept of **entropy**.

*Note: This chapter requires a good grasp of some key concepts in information theory, namely [**surprisal**](#surprisal), [**entropy**](#entropy), [**cross-entropy**](#cross-entropy), and [**KL divergence**](#kl-divergence). If you need an introduction, check out our [Primer on Information Theory](#informtaion-theory-primer-1) in the appendix.*


## Informative Notes {#informative-notes}

First of all, we consider a post and a note and the two probability distributions denoting two distinct **upvote rates**^[as introduced in the [previous chapter](#estimating-upvote-rates)]:
Let $P$ be the upvote rate of the post *without* the note, and $P^*$ the upvote rate of the post *with* the note.
The problem we are addressing is to calculate the **magnitude of the information asymmetry** between users that voted on the post *without* the note and the users that voted on the post *with* the note.
We say that the note is **informative** if the upvote rate $P^*$ is different from $P$.

### Votes and Surprisal

Let's say we have a post with upvote rate $p$.
Then when we register a new vote, our surprisal about the outcome will be $-\log(p)$ if the vote is an upvote and $-\log(1-p)$ if the vote is a downvote.
This means that if, for instance, $p = 0.99$ (i.e., the post has a very high upvote rate), then we will be very surprised if we see a downvote, and not very surprised if we see an upvote.
If we see an upvote, our surprisal will be $-\log(0.99) = 0.014$ bits (very low), and if we see a downvote, our surprisal will be $-\log(0.01) = 6.64$ bits (very high).
If $p = 0.5$, then we will be equally surprised by an upvote or a downvote, since $-\log(0.5) = 1$ bit in both cases.

Now, what would it mean if we were constantly surprised by the votes we see?
It means that our current estimate of the upvote rate is not very good.
In the long run, we can **take the average of our surprisals** to get a measure of how good our estimate is.
This is equivalent to taking the **expected value** of the surprisal, which is the **entropy** of the distribution of votes.
In a way, we can use entropy as a **measure of error**.
So our average error is...

$$
H(p) = -(p \cdot \log(p) + (1-p) \cdot \log(1-p))
$$

... which is **the entropy of the upvote rate**.

As entropy is normalized by the number of votes, we can get the total error by multiplying it by the number of votes:

$$
\text{totalError} = n \cdot H(p)
$$


### The Meaning of Cross-Entropy

If we have a post and a note and the upvote rates $p$ for the post *without* the note and $p^*$ for the post *with* the note, we can calculate the **cross-entropy** $H(p^*, p)$:

$$
\begin{aligned}
H(p^*, p) &= -\sum_{x \in X} p^*(x) \cdot \log(p(x)) \\
          &= -(p^* \cdot \log(p) + (1-p^*) \cdot \log(1-p))
\end{aligned}
$$

As we believe $p^*$ to be closer to the "true" upvote rate (because we assume that users who saw the post with the note to be **more informed**), we can interpret $H(p^*, p)$ as our estimate for how much we err on average if we use the previously estimated (uninformed) upvote rate.

$H(p^*,p)$ will always be greater than $H(p)$ if $p \neq p^*$. That is, given new information (the note), we assume that our error is always greater for the uninformed estimate than for the informed one.
This is due to a very powerful property of surprisal as a measure of error:

**Expected surprisal is minimal when the estimated probability equals the actual frequency of the event.**

So over- or underestimating the probability always results in more error in the long run. Since $H(p^*,p)$ tells us what our average error *would* be in the long run, if $p^*$ were the true frequency and we still estimated the upvote rate using $p$, $H(p^*,p)$ is minimized when $p^*$ equals the previous upvote rate $p$, or when $H(p^*,p)=H(p,p)=H(p)$. In that case, the note would not be informative at all.


### Total Relative Entropy = Cognitive Dissonance

The difference between $H(p^*,p)$ and $H(p)$ is **relative entropy**, also known as the Kullback–Leibler divergence or KL divergence. Since $H(p^*,p) >= H(p^*)$, **relative entropy is never negative**, regardless of whether $p$ is greater or less than $p^*$.

If we take Bob's measure of Alices's total error, minus his measure of his own total error, we get the total relative entropy: how much **more** error Bob expects for Alice than he would expect if Alice knew more. 

The total relative entropy is our measure of cognitive dissonance.

$$
\begin{aligned}
cognitiveDissonance &= n \cdot H(p^*,p) - n \cdot H(p*) \\
                    &= n \cdot (H(p^*,p) - H(p*)) \\
                    &= n \cdot D_{KL}(p^* || p)
\end{aligned}
$$



## Detailed Example

Suppose a post without a note is given receives 900 upvotes and 100 downvotes. But a certain note, when shown along with the post, reduces the upvote probability to 20%.

The total cognitive dissonance is

$$
\begin{aligned}
        cognitiveDissonance    &= votesTotal × D_{KL}(p || q) \\
                               &= votesTotal × ( p × {\lg \frac{p}{q}} + (1-p) × {\lg \frac{1-p}{1-q}} ) \\
                               &= 1000 × ( .2 × lg~\frac{.2}{.9} + .8 × lg~\frac{.8}{.1} )  \\
                               &= 1966.01~bits
\end{aligned}
$$

Suppose that, after showing the note to users that already upvoted the post, 70 users change their upvote to a downvote. 

The new upvote probability $q_1$ will be approximately

$$
\begin{aligned}
    q_1  &≈ \frac{upvotes - 70}{votesTotal} \\
       &= \frac{900 - 70}{1000} \\
       &= .83 
\end{aligned}
$$

The new cognitive dissonance will therefore be

$$
\begin{aligned}
    cognitiveDissonance_1 \\
    &= votesTotal × DKL(p, q_1)\\
    &= votesTotal × DKL(.2, .83 ) \\
    &= 1376.95~bits
\end{aligned}
$$

So cognitive dissonance was decreased by $1966.01-1376.95 ≈ 589$ bits. Or in other words, 589 of information was gained.

The table below shows how relative cognitive dissonance falls as users change upvotes to downvotes, reaching zero when $p=q_t$ and therefore $upvotes=p×votesTotal=200$.

```{r}

lg = function(n) {
  return(log(n,2))
}

cross_entropy = function(p,q) {
  if (p == 0) {
    return (-lg(1-q))
  }
  if (p == 1) {
    return (-lg(q))
  }
  return (- p * lg(q) - (1-p) * lg(1-q))
  
}

dkl = function(p, q) {
  return (cross_entropy(p,q) - cross_entropy(p,p))
}

library(dplyr)
library(tidyr)
library(ggplot2)

library(knitr)

p = 0.2
q0 = 0.9
votesTotal = 1000
steps = 10
voteRate = votesTotal / steps
step = 0:steps
attentionTotal=10000
deltaAttention = attentionTotal/steps
voteChangeRate = votesTotal*(p-q0)/attentionTotal
upvotes = votesTotal*q0 + step*deltaAttention * voteChangeRate
q = upvotes/votesTotal
dissonance = votesTotal * dkl(p,q)
value = dissonance[1:steps] - c(dissonance[2:steps],c(0))
totalDissonance = dissonance[1]

#df = data.frame(weightedImpressions=step*deltaAttention, q_t=q, upvotes=upvotes, dissonance=dissonance,informationValue=c(NA,value))
df = data.frame(q_t=q, upvotes=upvotes,dkl=dkl(p,q), dissonance=dissonance,informationValue=c(NA,value))

knitr::kable(df, format="markdown")

```


```{r, fig.width=6, fig.height=5}

plot = ggplot() +
  geom_line(aes(x = upvotes, y = dissonance), color = "firebrick") +
  labs(
    title = "Cognitive Dissonance Falls (Information is Gained) as votes change",
    x = "Upvotes",
    y = "Cognitive Dissonance: votesTotal * D_KL(p || q_t)",
  ) 


if(p < q0) { plot = plot + scale_x_reverse() }
plot


```

Note how the rate of reduction of cognitive dissonance falls as $q$ approaches $p$.

Focusing attention on posts and notes that maximize the rate of reduction of cognitive dissonance is the overall goal of the global brain algorithm. See the next document on [information value](#information-value).


## Discussion

### Parallel to Machine Learning

Cross entropy is commonly used as the cost function in many machine learning algorithms. A neural network for example takes an input with labels (e.g. images of cats and dogs) and outputs an estimated probability (e.g. that the image is a cat). The cost function computes how far these probability estimates are from the correct labels, and the neural network is trained by minimizing the cost function.

If $ŷ_i$ is the machine's predicted probability for training example $i$, and $y_i$ is the correct output (1 or 0), then the total cross entropy cost is:

$$
    \sum_i H(y_i, ŷ) = \sum_i y_i × -{\lg ŷ_i} + (1-y_i) × -{\lg(1 - ŷ_i)}
$$


In our case, if we say that $y_i$ are users votes, and $ŷ_i$ is always equal to the uninformed users prediction $q$, then our cost function is identical to the cost function used when training a neural network:

$$
\begin{aligned}
    \sum_i H(y_i, q) &= \sum_i y_i × -{\lg ŷ_i} + (1-y_i) × -{\lg(1 - ŷ_i)} \\
                     &= \sum_i y_i × -{\lg q} + (1-y_i) × -{\lg(1 - q)}  \\
                     &= upvotes × -{\lg q} + downvotes  × -{\lg(1 - q)} \\
                     &≈ votesTotal×H(p,q) \\
\end{aligned}
$$

So both neural networks and the global brain "learn" by reducing cross entropy. The difference is that the global brain reduces entropy not by learning to make better predictions, but by in a sense teaching users to make better predictions of how a fully-informed user would vote.

### A Subtle Point

Note that cross-entropy in our case is a measure of the total surprise of uninformed users at the hypothetical future votes of informed users. That is to say, it is:

$$
\begin{aligned}
    &votesTotal×p × -{\lg q} \\
    &+ votesTotal×(1-p) × -{\lg (1-q)} ) \\
    &≈ hypotheticalInformedUpvotes × -{\lg q} \\
    &+ hypotheticalInformedDownvotes × -{\lg (1-q)} ) \\
    &= votesTotal×H(p,q)
\end{aligned}
$$

And not

$$
\begin{aligned}
    & votesTotal×q × -{\lg q} \\
    &+ votesTotal×(1-q) × -{\lg (1-q)} ) \\
    &≈ actualUpvotes × -{\lg q} \\
    &+ actualDownvotes × -{\lg (1-q)} ) \\
    &= votesTotal×H(q)
\end{aligned}
$$

This is potentially confusing (it caused me a great deal of confusion initially) because we are measuring the error of Alice's estimated probability $q$ with respect to hypothetical events that have have not yet actually occurred. But shouldn't we be measuring the error of the votes that have occurred?

No, because surprisal is a measure of the error of a probability estimate, no the error of the event. The events are "what actually happens" or, in case of uncertainty, "what we think will actually happen" given our best current estimate $p$.

Measuring error against actual vote events just gives us $votesTotal*H(q)$, which doesn't tell us how much actual votes differ from what they should be if users were more informed.

$votesTotal×H(p,q)$ makes the most sense as a measure of the tension between the current state of user's beliefs, and what that state **should** be, which is a hypothetical future where all users have changed their upvotes because of the information in the note, and thus $actualUpvotes = hypotheticalInformedUpvotes$ and $actualDownvotes = hypotheticalInformedDownvotes$.










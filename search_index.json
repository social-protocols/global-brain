[["index.html", "The Global Brain Algorithm About", " The Global Brain Algorithm Last updated: 2024-02-17 About Falsehood flies, and truth comes limping after it, so that when men come to be undeceived, it is too late… – Jonathan Swift The Global Brain algorithm converts a social network into a distributed brain, where groups of individuals act like neurons that process information and pass it on to others, resulting in a whole that is more intelligent than the individual human parts. The algorithm works by identifying information that is likely to change minds, and modeling how those changes propagate through the network. It then uses this information to focus attention on posts that reduce “cognitive dissonance” – difference of opinion due to people being exposed to different information. The result is a social protocol that drives productive conversations, maximizes the flow of useful and reliable information, reduces the flow of misinformation, and increases human alignment. This document contains write-ups and explanations on the Global Brain algorithm developed by the social protocols organization. This is a living document and the information in it may become outdated fast because the algorithm is still under active development. Nonetheless, we try to maintain up-to-date explanations of our ideas here. We will introduce the building blocks of our algorithms: simple, but important concepts which make up the foundation of the components from which the new platform is built. A proof of concept of this algorithm is being developed in The Social Network of the Future. We are grateful for feedback. Do not hesitate to send an email. "],["Rationale.html", "1 Rationale", " 1 Rationale Humans lie and humans pass on misinformation, whether inadvertantly or with the goal to manipulate others. No algorithm will ever change this. However, misinformation is harmful in social networks only when it is unchecked. Lies are only harmful if people don’t see the refutations. In fact, given that lies and misinformation will always be part of any online information eco-system, seeing lies along with rebuttals might actually be useful because people would have seen them anyways and thus become aware of the lies currently in circulation. The engagement-based algorithms in today’s social networks encourage the unchecked spread of rumors, regardless of how well they are supported, whereas the Global Brain algorithm discourages the spread of information that people are unlikely to upvote if they were fully informed, and encourages the spread of information that might check the spread of a false information that is already spreading. And yet the purpose of the Global Brain algorithm is NOT to tell people what is true. It is simply to direct people’s attention to information that may change their votes. But why do we want to change votes? An algorithm that tries to change minds sounds sinister. Propaganda changes how people vote. But propaganda works by selectively exposing users to information with the goal of changing opinions about specific things, all the while actively omitting contrary information. It is one sided and dishonest. The Global Brain, on the other hand, has no agenda other than driving productive conversation by maximizing the informedness users base their voting decisions on. The Global Brain algorithm drives a fair, unbiased process of weighing all the arguments that anyone cares to make. As long as there is sufficient intellectual diversity among recipients, the result is an adversarial process, where all the relevant information is exposed and processed. "],["problem-discussion-threads.html", "2 The Problem with Conventional Discussion Threads 2.1 A Novel UI to Navigate Discussion Threads", " 2 The Problem with Conventional Discussion Threads Discussion threads are essentially tree graphs. They have a root post which can have any number of replies, each reply can have any number of replies itself, and so on. Discussion trees are usually projected to a linear structure, where each reply is displayed below the post it replies to. The next level of depth is indicated by indentation, sometimes guided by vertical rulers that provide some visual orientation. The problem with this approach is that it heavily favors shallow discussions. It discourages following threads for longer than a few levels because any coherent thread is scattered across the UI and the context is easily lost. Consider the following two examples: In both cases, the thread is distributed over the linear space in a way that makes it hard to follow (and this is even a tame example). On the other hand, elements that have no coherence among them (replies on the same level) are often displayed close to one another. There are several fundamental problems with this approach: Breadth over depth: Threads are hard to follow because the context is lost as soon as there are several replies. This is especially true for longer threads. There is a lot less friction scrolling through the direct replies of any particular post, rather than following the discussion thread more deeply. Sensitivity to social cues: This setup is prone to users being influenced by social cues and it creates a favorable environment for opinion being swayed by majority sentiment rather than good arguments. Phenomena such as astroturfing and dogpiling flourish in such an environment. Unchecked misinformation: As people often gloss over threads getting a brief impression of what other people think about a certain piece of media, misinformation often remains unchecked. You can just post a piece of false information and it is likely read much more often than it is checked. Rebuttals cannot keep pace with misinformation. 2.1 A Novel UI to Navigate Discussion Threads We propose a novel UI to navigate discussion threads. The basic elements of the UI are the following: Posts: A piece of media submitted to the platform by a user, just like posts on any other social media platform. Posts can either be top-level posts or replies to other posts, thus forming conversation threads. Note: A post’s top reply (we also call it a post’s note1) is a reply that is shown alongside it to provide helpful context. Votes: Users evaluate content through upvotes and downvotes. The details page for a post looks like this: The post’s parents are displayed above it so that the context of the conversation is not lost2. Other replies are displayed below the post. This is how the mapping between the discussion tree and the UI looks like: The coherence in a discussion thread is preserved and following a discussion thread is almost like reading a chat log. Navigating a discussion thread looks like this: Notice that we are not displaying all replies at once in our UI. This means that we distribute more attention to certain replies than others. Other services focus attention on top replies, but only if users actually click “show replies”. This tends to cut the discussion off at the top level of the tree. In contrast our UI always prompts the user with a reply at the next level of depth, constantly encouraging users to go deeper in the discussion tree. But this raises the question: How do we decide which replies receive more attention and which replies receive less? In reference to X’s community notes which we took inspiration from.↩︎ There is no parent thread when the post is a top-level post, i.e., the root of the discussion.↩︎ "],["key-concepts-and-assumptions.html", "3 Key Concepts and Assumptions 3.1 Establishing Causality 3.2 Distributed Reasoning 3.3 Optimizing for Information Value 3.4 Reducing Cognitive Dissonance 3.5 Cognitive Dissonance as Relative Entropy 3.6 The Causal Model", " 3 Key Concepts and Assumptions 3.1 Establishing Causality The basic computational unit in the Global Brain is the informed vote. Before users vote on a post, the UI shows them a note and then records their vote, given they were shown that note before voting. If the informed vote is different from the uninformed vote, it means the note caused changes in votes. We call notes that do change behavior informative. Every reply to every post becomes effectively an AB test that tells us how exposure to a note affects votes on a post. What makes properly-run AB tests very powerful is that they establish causality. They tell us not just that upvotes on the note and post are correlated, but that the note actually causes changes to votes on the post. Establishing these causal links allows us to model the Global Brain as a causal Bayesian network. The causal links between notes and posts are the synapses of the Global Brain. They allow us to predict how the average user would vote if they were exposed to some combination of notes and posts, by predicting how exposure to those posts will influence upvotes on other posts, and how that influences upvotes on other posts, and so on. 3.2 Distributed Reasoning By simulating how beliefs propagate through the network, the Global Brain engages in a form of reasoning, loosely defined. Each neuron in the Global Brain reasons – or processes information – using the same inferences the average user makes. But unlike the average user, the Global Brain can process all information in the system in a valid Bayesian manner. So in a way, it simulates an average user who has unlimited time, memory, and processing capacity. We can then query this model to estimate how the Global Brain would vote if it was exposed to all information – all posts ever made. We can also use the model to identify where to direct users’ attention in order to reduce cognitive dissonance in the network. Example: Did an Earthquake Just Happen? Suppose there is a post A with a video showing the aftermath of a recent earthquake. This post goes viral, but then somebody posts a note B on the video which says “This video is actually from an earthquake in 2004. Here is a link to the original video.”. Suppose that people who are shown note B along with the video in post A are less likely to upvote A. This will often be the case, as many people probably upvoted out of simple ignorance, not malicious intent to spread misinformation. The Global Brain will conclude that: B is informative. A should receive less attention, because informed users are unlikely to upvote it. B should receive more attention from users who have already seen or upvoted A in order to reduce cognitive dissonance. Now, suppose that it turns out the information in note B is actually wrong. Somebody may reply to B with a subnote C saying “No, the video in that link is completely different! This video is recent footage from CNN. Here’s a link.” The Global Brain can actually look at how C affects users’ votes on B to estimate how a fully informed user would have voted on A. Even if there is no fully informed user who saw post C and then changed their vote on A, we can in a sense simulate a fully-informed user. This is what allows the Global Brain to act like a kind of neural network, with the links between posts acting as synapses. No matter how many posts there are in the system, we can simulate the actions of a user who has read every post ever posted in the network – in other words, a user who has processed all available information. For example, if post B links different groups of users in different conversations – for example if B is “reposted” or cross-posted to more than one forum – then the Global Brain can observe how C affects B in one forum, and how B affects A in another forum, and “link” the two forums, predicting the probability that a fully-informed user would upvote A after seeing subnote C, even if no user who voted on A actually saw subnote C. 3.3 Optimizing for Information Value A key assumption we make is that more information leads to better judgments. Even though people can post false and misleading information, people can respond by explaining why this information is false or misleading. Even though people can promote hateful or harmful ideas, people can respond with reasons why these ideas are wrong. By identifying information that changes minds, driving attention to that information, and then identifying responses that change minds back, the system drives informative conversations. Although this isn’t guaranteed to lead to the truth, it will at least result in more informed users. And it will also help identify the most informed opinions: what posts would still be upvoted by users who were exposed to all the strongest information on arguments on all sides. In the example above, once the model learns that B is changing people’s votes on A, it directs more attention to B, resulting in fewer upvotes for A. We can think of B as an argument against A, or a reason not to upvote A. The “reason” doesn’t need to take any particular logical or rhetorical form. It is a reason by virtue of being the literal reason for – the observed cause of – changes to votes. Unfortunately, in the example above, B was not true. But fortunately, C is a convincing reason to believe that B is not true. C actually causes people not to change their vote on A, in spite of being exposed to B. B and C cancel each other out, so to speak. So the Global Brain concludes that it is no longer helpful to direct attention to B, and that it should continue to direct attention to post A. 3.4 Reducing Cognitive Dissonance The Global Brain only requires a small number of people to actually change their minds in order to learn how beliefs affect other beliefs and construct the causal Bayesian network. So the predictions of the model may differ significantly from the beliefs of the majority. Suppose 1000 users voted on a post and most votes were upvotes. Then suppose that among a subset of 30 users who saw some very informative note, the probability of upvoting the post dropped to close to zero. We can thus estimate that if all 1000 users saw that note, they would not have upvoted the post. Even though in actuality most users did upvote it. In such situations, we say there is a large amount of cognitive dissonance in the network. Cognitive dissonance arises whenever people’s upvotes differ from what their upvotes would be if they were exposed to all the relevant information. Or in other words, when people’s votes differ from the predictions of the causal model. The goal of the Global Brain algorithm is minimizing cognitive dissonance. It does this by exposing users to the informative note, reducing the inconsistency between what people upvote and what they would upvote if they were more informed. Reducing cognitive dissonance brings individual participants into greater alignment, reducing differences of opinion that are due to differences in information. It cannot of course eliminate differences of opinion. Opinion can be subjective, because people have different priors and different ways of processing information. But it can identify situations where differences are due to ignorance of readily available information: when one group of people believe something that another group doesn’t only because they haven’t seen the same posts. The result is a network that learns. The model learns as new posts are submitted to the network. And the human participants in the network also learn as they are exposed to posts that bring their mental models into greater alignment. 3.5 Cognitive Dissonance as Relative Entropy We can measure cognitive dissonance as the relative entropy (or KL divergence) between users’ actual beliefs and their hypothetical fully-informed beliefs (the beliefs they would have if exposed to all posts). Relative entropy is a way of measuring the distance, or error, between two probability distributions. It is similar to the measure used as the loss function when training a neural network, and in this sense, the Global Brain can be seen to “learn” in a sense similar to how a neural network learns. Optimizing for reducing cross-entropy is sufficient to drive productive conversations, because it directs attention exactly where it needs to be directed. Consider again the example above, where subnote C “cancels out” the effect of note B on post A. This means that there is no difference between users beliefs about post A and their fully-informed beliefs, given they have seen B and C. So relative entropy is zero, and there is no value to be gained from directing more users’ attention to note B. On the other hand, since some users have already seen B and had their minds changed, there is a difference between their probability of voting on post A and the fully-informed probability. So there is still cognitive dissonance among the subset of users who were exposed to B. Exposing that subset of users to post C, until the upvote percentage of that subset matches the overall upvote percentage, will result in minimal cross-entropy. The theory and math for calculating cross-entropy and ranking posts based on their potential to reduce cross-entropy is developed in Cognitive Dissonance. 3.6 The Causal Model The Global Brain requires modeling users’ beliefs as a causal Bayesian network. But making this link is tricky, because we don’t actually know what users believe, only how they vote. Suppose we know how much exposure to post B changed the probability that users will upvote post A, and how much exposure to post C changes the probability that a user will upvote post B. Can we predict how much exposure to post C will change the probability that a user will upvote post A? We can draw a causal graph with our assumptions of how information causes changes in votes. We assume that votes are influenced by hidden variables, which are users’ actual beliefs. For example, when somebody posts B, and it influences their vote on A, B must contain some information that users didn’t already have, and that gives them a reason to change their vote. If B did not contain new information, why would it change their behavior? Even a humorous comment can be modeled as information for our purposes. Suppose somebody responds to post A with a purely humorous comment B and this causes more people to upvote A. Why would a joke cause more people to upvote A? Since we assume votes indicate intent to direct attention, it can only mean that the fact that joke B was funny caused people to believe that A should get more attention. Maybe it convinced them that A could be an amusing topic. But we don’t need to have a theory of how people reason or what it is in people’s minds that cause them to change their behavior. It is sufficient to assert that there is some latent variable that links exposure to a post to voting behaviors. To understand this process, it helps to think in purely Bayesian terms. We can say that if something causes people to change votes, it is a reason for the change of votes and that these reasons are unobserved “underlying beliefs” in some proposition. Let’s say 𝑍 represents users’ underlying belief that was directly changed by the post B (e.g. B is a video of Bigfoot, which causes users to change their belief in 𝑍=there is video evidence that Bigfoot exists). The belief in 𝑍 has a causal effect on votes on B. 𝑍 also has a causal effect on some other underlying belief 𝑌 (e.g. 𝑌=Bigfoot exists). Finally, the underlying belief 𝑌 has a causal effect on upvotes on post A (“Bigfoot is real, people!”) Then let’s use the term 𝑆𝑏 and 𝑉 to represent the event that users were exposed to posts B and A respectively. And finally we’ll use italic 𝐵 and 𝐴 to represent upvotes on B and A respectively. So showing a user post B (𝑆𝑏) affects belief in 𝑍 which affects votes 𝐵, and so on. And belief 𝑍 also affects belief 𝑌. So our causal graph looks like this. 𝑆𝑏 → 𝑍 → 𝐵 ↓ 𝑌 → 𝐴 Now, let’s assume that the number of upvotes 𝐴 or 𝐵 is proportional to the number of people that believe 𝑌 or 𝑍 respectively. \\[ P(A = 1) \\propto P(Y = 1) \\] \\[ P(B = 1) \\propto P(Z = 1) \\] So we can kind of use 𝐴 and 𝐵 as proxies for 𝑌 and 𝑍. Now, suppose a user responds to post B with post C That video was generated by AI!. Seeing post C (𝑆𝑐) indirectly causes a change in the probability that users upvote A. And for the moment, assume that 𝑆𝑐 only affects 𝑌 through 𝑍. So seeing post C affects the belief that 𝑍=there is video evidence that bigfoot exists which affects the belief 𝑌=bigfoot exists which affects 𝐴. Our causal graph looks like this: 𝑆𝑐 → 𝐶 ↓ 𝑆𝑏 → 𝑍 → 𝐵 ↓ 𝑌 → 𝐴 Now, what can we do with this causal graph? Well, assuming the causal assumptions in the graph are true, we can deduce how 𝑆𝑐 will affect 𝑌 (through 𝑍), without directly observing either 𝑌 or 𝑍 (the hidden states in users’ minds), simply by observing how 𝑆𝑐 affects 𝐵 and how 𝑆𝑏 affects 𝐴. This means that we can model long chains of “reasoning” and predict how one piece of information would propagate through the Global Brain, even if there is no individual who actually engages in that chain of reasoning. For example, post B (the video of bigfoot) might be cross-posted to a forum dedicated to identifying AI-generated videos. If comments in that forum convince users in that forum that the video was AI generated, then the Global Brain can deduce that some people in the cryptobiology forum would also be convinced. The groups of users in the two forum don’t have to overlap for information to flow between forums. This means that for very complex issues, where no individual can come close to being able to process all the information in the network, the Global Brain can essentially simulate a fully-informed individual by breaking the problem down into parts and propagating information between conversations using posts that are “cross-posted”. "],["driving-informative-discussions.html", "4 Driving Informative Discussions 4.1 An Informal Argument Model 4.2 What is the “Best” Reply?", " 4 Driving Informative Discussions The most important component of the Global Brain protocol on the thread level is the algorithm that identifies the best reply for any post and promotes it to be its note. If we do this for all posts and replies, the result is a system of tree-style discussions where users can follow the most informative conversation in any thread by following the best replies on any post. Of course, users don’t have to follow the top thread. At any point, they might decide that another answer is more interesting or helpful to them and follow that thread instead. 4.1 An Informal Argument Model The Global Brain algorithm’s core objective is to maximize the impact of the best ideas and arguments. We do this by finding the “best” reply to any post to make it its note – the most helpful context shown alongside it. But “best” is a tricky term in the realm of arguments and ideas. What makes an argument “best”? And how do we even know what an argument is if everything is just a post? Many attempts have been made to formalize debate by creating formal models of claims, propositions, arguments, and other constituents of debate. We deviate from this approach and propose an informal argument model. The Global Brain does not require a formal model of belief. It doesn’t need posts to be structured as formal propositions or claims. It doesn’t need to know if users agree or disagree with posts. The only assumptions are: Upvotes and downvotes define a single dimension across which content is evaluated and there is a common understanding among users about what upvotes and downvotes mean. The direction of Users’ votes is causally related to their underlying beliefs about the topic of the post that we cannot observe (latent variables). By making these assumptions about the causal relationship between beliefs and votes and then watching how exposure to notes causes changes to votes, we can model underlying beliefs and predict how exposure to a note will cause changes to these beliefs, which will cause changes to other beliefs, and so on. 4.2 What is the “Best” Reply? The best reply in our approach is the most convincing one. We quantify the degree to which it is convincing by measuring which reply most likely changes the voting behavior on that post. First of all, we need to observe the voting behavior on posts. We do this by keeping a tally of upvotes and downvotes for each post. In addition to that, we also keep track of tallies of post/note combinations. This means that we keep track of how many upvotes and downvotes each post received when a note was shown alongside it. By comparing the voting behavior on a post with and without a note, we can estimate the causal effect of showing a note on the voting behavior on that post. In the next chapter, we will introduce how we estimate upvote probabilities from these tallies. "],["modeling-upvote-probability.html", "5 Modeling Upvote Probability 5.1 The Beta-Binomial Model 5.2 Naive Point Estimate 5.3 The Bayesian Average 5.4 The Bayesian Hierarchical Model", " 5 Modeling Upvote Probability Given a tally of votes at a certain point in time, we can calculate the upvote ratio at that time \\(t\\) which is given by \\(upvotes:downvotes\\). We can also express it as the fraction of upvotes over all votes: \\[ \\frac{upvotes}{upvotes + downvotes} \\] You can think about the upvote fraction as an average vote or as the probability of a vote being an upvote (we will mostly call this the upvote probability). We model votes as a Bernoulli distributed random variable with successes given by upvotes and failures given by downvotes. If we encode upvotes with a value of \\(1\\) and downvotes with a value of \\(0\\), a sample post’s gathered votes at a certain point in time \\(t\\) might look like this: \\[ (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) \\] The post has gathered a total of 12 votes of which 3 are upvotes and 9 are downvotes. The upvote fraction or the average vote for this sample is: \\[ \\text{upvoteFraction} = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{upvotes}{upvotes + downvotes} = \\frac{3}{3 + 9} = 0.25 \\] 5.1 The Beta-Binomial Model We assume that each post has a “true” upvote probability that we can only estimate by collecting votes. Votes are modeled as a Bernoulli random variable with parameter \\(p\\) that denotes our upvote probability. Upvote probility is modeled as a Beta distribution. The Beta distribution has two shape parameters \\(\\alpha\\) and \\(\\beta\\) which in our case are given by our upvote and downvote counts. \\[ u_i \\sim \\text{Bernoulli}(p) \\\\ p \\sim \\text{Beta}(upvotes, downvotes) \\] Example To provide a more intuitive understanding of how this distribution models our upvote probability, let’s see how our beliefs about the true upvote probability develop for an example post when we collect more and more votes. Let’s say we have a post with a true upvote probability of \\(0.24\\). We initialize the distribution with a prior of 1 upvote and 1 downvote. This is equivalent to a uniform distribution which means we assign equal weight to any upvote probability. Now, we observe some votes. \\[ (\\color{limegreen}{1, 0, 0}) \\] \\[ (\\color{grey}{1, 0, 0,} \\color{limegreen}{0, 1, 0}) \\] \\[ (\\color{grey}{1, 0, 0, 0, 1, 0,} \\color{limegreen}{0, 0, 0}) \\] \\[ (\\color{grey}{1, 0, 0, 0, 1, 0, 0, 0, 0,} \\color{limegreen}{1, 0, 0}) \\] As we update our beliefs, the probability mass becomes more concentrated which means we become more and more certain about our beliefs about the true upvote probability. If the post would develop further with a similar trajectory, the following could be the outcome after 500 votes: Our probability mass is now pretty concentrated around (what in this case we know to be) the true upvote probability. 5.2 Naive Point Estimate Now that we can express our beliefs about the upvote probability, how do we make a “best guess” at any given point in time? A naive way of estimating the upvote probability is to take the actual current ratio \\(upvotes:downvotes\\), or rather the fraction, which is equivalent to the plain sample average vote: \\[ P_t(upvote) = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{upvotes}{upvotes + downvotes} \\] This solution is naive because it ignores an important fact: In the beginning, we do not take into consideration any prior information about the upvote probability of the post. Thus, when the post does not yet have a lot of votes, each new arriving vote has an outsized effect on our estimate that only gets increasingly small over time. This would result in erratic estimates in the beginning which would only smooth out over time. Here is how the cumulative mean develops for a random vote history with a true upvote rate of \\(0.6\\): If we were to use this metric to compare posts, getting a high upvote probability estimate in the early stages of a post would essentially come down to luck. It will temporarily fare overly well or overly poorly compared to other posts and the estimate will only get better over time once we have more votes on the post. We have to take into account prior information to avoid this. 5.3 The Bayesian Average The Bayesian Average uses a weighted prior estimate of the average to avoid the erratic shifts in the estimate when there is not a lot of data. It is calculated as… \\[ \\frac{\\color{blue}{C \\cdot m} + \\sum_{i=1}^n x_i}{\\color{blue}{C} + n} \\] … where \\(C\\) is a weight constant and \\(m\\) is our prior belief about the average. If you compare it to the plain average, we simply add \\(C \\cdot m\\) to the nominator and \\(C\\) to the denominator. But what does this achieve in concrete terms? Let’s build up to this formula step by step for estimating the true upvote probability of a post. First, remember that the sample upvote fraction can be thought of as the “plain” average vote. In our case, this means: \\(\\sum_{i=1}^n{x_i} = upvotes\\) is the number of upvotes \\(n = upvotes + downvotes\\) is the total number of votes Substituting the values in the Bayesian average formula gives us: \\[ \\frac{C \\cdot m + \\sum_{i=1}^n x_i}{C + n} = \\frac{C \\cdot m + upvotes}{C + upvotes + downvotes} \\] Adding these terms to our formula means that we calculate the cumulative average as if we had collected \\(C\\) votes with an upvote rate of \\(m\\) before we collected the first vote on our post. If our prior belief about the average is \\(0.68\\) and we chose a weighting factor of, say, \\(100\\), this would mean that we calculate the average as if we had previously collected \\(100\\) data points which amounted to an upvote fraction of exactly \\(0.68\\). Plugging in the values makes this apparent: \\[ \\frac{100 \\cdot 0.68 + upvotes}{100 + upvotes + downvotes} = \\frac{68 + upvotes}{100 + upvotes + downvotes} \\] Graphically, it looks like this (the light grey line is the plain average for comparison): The cumulative Bayesian average is much less erratic when little data is available. However, there is an important question left: How do we chose a good prior and a good weight? Here is how the Bayesian average develops for different prior beliefs about the average. Bayesian averages over time are indicated by light grey lines, the priors chosen here are \\(0.1\\) through \\(0.9\\) and the weight is kept constant at \\(20\\). The plain average is overlayed in darker grey for reference. And here is that same plot with the same priors, but with a weight of \\(100\\). 5.4 The Bayesian Hierarchical Model We can estimate the global priors \\(C\\) and \\(m\\) using a Bayesian Hierarchical model. Our beliefs about the upvote probability for each post can be modeled using a Beta distribution. Our prior beliefs for each post are the same: a Beta distribution with a mean \\(m\\) and sampleSize \\(C\\). As we collect data for each post, we can use this not only to update our beliefs about the post, but also to update our beliefs about the global prior. After collecting a lot of data for a lot of posts, we will obviously have a good idea of what the global mean \\(m\\) is. But how do we estimate \\(C\\)? To be able to form and update beliefs about C using a Bayesian approach, we need to have priors beliefs about C. The same for \\(m\\) as a matter of fact. Our priors \\(m\\) and \\(C\\) are our beliefs before we have any data at all. It’s reasonable for the global hyperprior for \\(m\\) to be a uniform distribution: any upvote probability is equally likely before we have any data about any posts. We also need a hyperprior for \\(C\\). This one is tricky. It’s essentially our prior beliefs about how much variation there will be in upvote probabilities for different posts. If it is very high, then no matter what data we observe for a post, the posterior for that post will be close to the global prior \\(m\\). It’s hard to say much about the hyperprior for \\(C\\) except that it should probably be a positive number and, by experience, between 2 and 10. A gamma or exponentially distribution is often used for this type of model (for reasons I should understand better). So we’ll choose a weak gamma prior and add 2. So we have a hierarchical model that looks like this: \\(m \\sim \\text{Uniform}(0, 1)\\) \\(C \\sim \\text{Gamma}(2, 2)\\) \\(q \\sim \\text{Beta&#39;}(m, C)\\) \\(Z \\sim \\text{Binomial}(n, q)\\) Where $q is the upvote probability for the post $Z is the number of upvotes on the post $n is total number of votes on the post \\(\\text{Beta`}\\) is a Beta distribution parameterized using mean and sample size instead of \\(α\\) and \\(β\\), using the conversions: \\(\\alpha = mean \\times sampleSize\\) \\(\\beta = (1 - mean) \\times sampleSize\\) Using methods such as MCMC, we can estimate the posterior of the entire joint probability distribution, giving us an estimate for the upvote probability for each post, and a mean and weight for the global prior. 5.4.1 Approximating the Mean Upvote Probability However, we don’t need to rerun the MCMC simulation every time we collect more vote data for a post. Once we have an estimate for \\(m\\) and \\(C\\), we can treat these as fixed constants. This let’s us chop of one level of our hierarchy. Now for each post, we have a simple standard Beta-Binomial model. \\(q \\sim \\text{Beta&#39;}(m, C)\\) \\(Z \\sim \\text{Binomial}(n, q)\\) Where \\(m\\) and \\(C\\) are constants. For a Beta-Binomial model is simply: \\[ Beta(α+upvotes,β+votes-Z) \\] The mean of the posterior is \\[ \\frac{α + Z}{α + β + n} \\] Which we can rewrite using the above conversions as: \\[ \\frac{sampleSize \\cdot mean + upvotes}{sampleSize + votes} \\] Using our globalPrior of \\(mean=m\\) and \\(sampleSize=C\\) gives us our formula for the Bayesian Average: \\[ \\frac{sampleSize \\cdot mean + upvotes}{sampleSize + votes} \\\\ = \\frac{C \\cdot m + upvotes}{C + votes} \\] "],["the-informed-upvote-probability.html", "6 The Informed Upvote Probability 6.1 Definition of Informed Upvote Probability 6.2 Extrapolating Informed Upvote Probability 6.3 Uninformed, Informed, and Overall Tallies 6.4 The Adding Informed Probability to the Hierarchical Model 6.5 The Reversion Parameter 6.6 Full Model 6.7 Calculating The Informed/Uninformed Upvote Probabilities", " 6 The Informed Upvote Probability 6.1 Definition of Informed Upvote Probability The informed upvote probability is the upvote probability given the user has been informed of a note on the post. The uninformed upvote probability is the upvote probability given they have not. A user has been “informed” of the note if they have read it and actually considered it. Now it is impossible to know what a user has actually considered. But, we can use the fact that the user either voted or replied to the note as a proxy. So we can define the informed upvote probability (on a post give a note) as the probability that a user votes on a post given they have voted on or replied to the note. 6.2 Extrapolating Informed Upvote Probability We can also use the fact that a user was shown the note at the time the voted on the post as an indicator that they may have considered the note, and it should be possible to extrapolate the informed upvote probability – the probability that a user upvotes given they have considered a note – based on estimates of the probability that a user upvotes a post given they were or were not shown the note, as proposed in these research notes. As of 2024-Feb this has not been implemented. 6.3 Uninformed, Informed, and Overall Tallies Uninformed and informed upvote probabilities are calculated based on tallies for uninformed and informed upvotes. These tallies are calculated by looking up what notes users have and have not voted on or replied to at the time the user votes on a post. These tallies may overlap, in the sense that a single user may be counted in both the uninformed and informed tallies. This is because a user can vote before becoming informed. There may also be users who are counted only in the uninformed tally (those who were never informed) or only in the informed tally (those who were informed before voting on the post). So the calculation of the uninformed and informed upvote probabilities involves three tallies: the informed, the uninformed, and the overall or final tally. The final tally is the sum of the other two iff the uninformed and informed users are mutually exclusive. For example suppose there are two voters overall. Both voters voted before becoming informed. One of them changed their vote. So are are two total votes in both the informed tallies. uninformed_tally = (upvotes=0, votes=2) informed_tally = (upvotes=1, votes=2) overall_tally = informed_tally The following scenario also has two voters overall. But there is only 1 informed user and 1 uninformed. The tallies in this case are mutually exclusive and the overall tally is just the sum of the others informed_tally = (upvotes=1, votes=1) uninformed_tally = (upvotes=0, votes=1) overall_tally = informed_t + uninformed_t = (upvotes=1, votes=2) 6.4 The Adding Informed Probability to the Hierarchical Model We can model our beliefs about the informed probability using a Beta distribution, as we did when modeling the uninformed upvote probability in The Bayesian Hierarchical Model. The simplified hierarchical model for modeling the uninformed probability is: \\(q \\sim \\text{Beta&#39;}(m, C)\\) \\(Z \\sim \\text{Binomial}(n, q)\\) It seems intuitive to base our prior for the informed upvote probability \\(p\\) on the uninformed upvote probability \\(q\\). For example, we might add this to our hierarhical model: \\(p \\sim q\\) However, this doesn’t work, because our prior beliefs about \\(p\\) are not actually equal to our beliefs about \\(q\\). For example, suppose the sample size for the uninformed tally is very large: say 900 upvotes out of 1000 votes. Then the posterior \\(q\\) will have a very large sample size parameter and thus will make for a very strong prior. It would take a lot of informed votes for us to be convinced that \\(p\\) is very different from \\(q\\). But suppose we then have 10 informed votes and they are all downvotes. Intuitively, this is already strong evidence that \\(p\\) may be quite a bit lower than \\(q\\). Yet if our prior sample size is 1000, adding a sample of 10 will not change it very much. So the prior sample size for \\(p\\) should not be based on the sample size for \\(q\\). Rather, this should be another global parameter \\(C2\\) which represents our beliefs about how different we expect \\(p\\) and \\(q\\) to be a priori. Or in other words, \\(C2\\) represents our prior beliefs about how likely people are to change their voting behavior after considering notes. So our updated model now looks like this: \\(q \\sim \\text{Beta&#39;}(m, C)\\) \\(p \\sim \\text{Beta&#39;}(mean(q), C2)\\) \\(Z_{uninformed} \\sim \\text{Binomial}(n, q)\\) \\(Z_{informed} \\sim \\text{Binomial}(n, p)\\) This model is simplified by treating \\(m\\), \\(C\\), and \\(C2\\) as constants. The full model needs to include our prior beliefs about these values. \\(m \\sim \\text{Uniform}(0, 1)\\) \\(C \\sim \\text{Gamma}(2, 2)\\) \\(C2 \\sim \\text{Gamma}(2, 2)\\) But as long as we have a lot of data for a lot of post and note combos, we only need to estimate the parameters of the full model once. We can then treat them as constants using the simplified model and we will get nearly identical results. 6.5 The Reversion Parameter But let’s question our assumption of centering our prior for \\(p\\) on the mean of \\(q\\). If \\(q\\) really our best estimate of the value of \\(p\\)? Suppose the estimated upvote probability on some post is \\(q=.9\\), and the global prior is \\(m=0.5\\). So we have above-average upvote probability. Then let’s say somebody adds a note. Do we expect, a priori, that the informed upvote probability will remain at \\(p=.9\\)? I don’t think so. I think that we actually expect the note to reduce the upvote probability. It is certainly more likely that the note reduces the upvote probability than increases it even more. Why is this? Well it’s sort of regression to the mean situation. Posts with high upvote probability are unusual. And often, they are too good to be true. They don’t stand up to scrutiny. They are click bait or misinformation that users wouldn’t have upvoted if they knew more. Often the reason people submit notes is to add this additional context, and this context may decrease the upvote probability. At least, it is more likely that it will decrease it than increase it. If there was information that would have increased the upvote probability even more, the poster likely would have included it in the original post. So we can model this belief with something I call the reversion parameter. Our expected value for \\(p\\) (which I’ll notate \\(p̅\\)) reverts to somewhere between \\(m\\) and \\(q̅\\), and the reversion parameter \\(r\\) tells us how much of the distance between \\(m\\) and \\(q\\) to revert. Specifically: \\[ p̅ = q̅ - r(q̅ - m) \\] If \\(r=0\\), we expect no reversion and \\(p̅ = m\\). If \\(r=0\\) we expect \\(p̅ = q̅\\). We can model \\(R\\) with a beta distribution with a very weak prior. So our new model adds we redefine our prior for \\(p\\): \\(r \\sim \\text{Beta}(1, 1)\\) \\(p \\sim \\text{Beta&#39;}(q̅ - r(q̅ - m), C2)\\) 6.6 Full Model In our full model, we have multiple posts and notes. We’ll designate the uninformed probability for post \\(i\\) as \\(q_i\\). And the informed upvote probability for post \\(i\\), given user is informed of note \\(j\\), is \\(p_{i,j}\\). So our full model is now: \\(m \\sim \\text{Uniform}(0, 1)\\) \\(r \\sim \\text{Beta}(1, 1)\\) \\(C \\sim \\text{Gamma}(2, 2)\\) \\(C2 \\sim \\text{Gamma}(2, 2)\\) \\(q_i\\sim \\text{Beta&#39;}(m, C)\\) \\(p_(i,j) \\sim \\text{Beta&#39;}(q̅_i - (q̅_i - m)×R, C2)\\) \\(Z_{uninformed} \\sim \\text{Binomial}(n, q_i)\\) \\(Z_{informed} \\sim \\text{Binomial}(n, p_{i,j})\\) 6.7 Calculating The Informed/Uninformed Upvote Probabilities Given this model, we can use a Monte Carlo simulation or other Bayesian methods to estimate \\(p\\) and \\(q\\) for each post. But Monte-Carlo simulations can take a long time to run. To make computation more efficient, we can run the full simulation only periodically on the full data set to estimate the values for \\(m\\), \\(r\\), \\(C\\) and \\(C2\\). We can then treat these as constants and run mini Monte-Carlo simulations for each post/note combination. Since Tallies usually involve very small sample sizes, these simulations run very quickly. On my 2024 Macbook M2 Max, I was able to run these simulations for every possible informed/uninformed tally combination having sample sizes up to 10 – a total of 4225 simulations – in about 5 minutes on a single thread, using Julia/Turing and the NUTS sampler. Since the vast majority of tallies will have small sample sizes (less than 10), we can gain a lot of speed by pre-calculation or memoization. Further, for larger sample sizes, I have developed an estimation formula based on the Bayesian average that is pretty accurate. "],["the-fully-informed-upvote-probability.html", "7 The Fully-Informed Upvote Probability", " 7 The Fully-Informed Upvote Probability The core algorithm on the thread level is the algorithm that identifies the top replies. It quantifies how much information each reply adds to the original post and determines the reply that adds the most information. A different way of putting it would be that we predict how a fully informed user would vote on any given post. We can then compare that informed upvote probability with the uninformed one to determine how convincing the reply is with regard to the original post. We are considering two posts: a post \\(A\\) and direct reply \\(B\\). We observe the following events: \\(u_A := \\text{a vote on A is an upvote}\\) \\(u_B := \\text{analogous}\\) \\(s_B := \\text{the user was shown post B}\\) \\(\\bar{s_B} := \\text{the user was not shown post B}\\) \\(s_{B*} := \\text{the user was shown the top note on B}\\) \\(\\bar{s_{B*}} := \\text{the user was not shown the top note on B}\\) What we are looking for is \\(P(u_A | s_B, s_{B*})\\), the probability of an upvote on \\(A\\) given that the user has seen \\(B\\) and the top note on \\(B\\) (fully-informed upvote probability). We define it as the weighted average of the upvote probability of \\(A\\) given that the user has seen the top note on \\(B\\) and the upvote probability of \\(A\\) given that the user has not seen the top note on \\(B\\): \\[ P(u_A | s_B, s_{B*}) = P(u_A | s_B) \\cdot \\text{support} + P(u_A | \\bar{s_B})(1 - \\text{support}) \\] The weights are given by the support of the top note on \\(B\\). Support is the ratio (expressed as a fraction) between the informed upvote probability of \\(B\\) given that the user has seen the top note on \\(B\\) and the informed upvote probability of \\(B\\) given that the user has not seen the top note on \\(B\\). \\[ \\text{support} = \\frac{P(u_B | s_{B*})}{P(u_B | s_{B*}) + P(u_B | \\bar{s_{B*}})} \\] As we condition on the user being shown the top note of post \\(B\\) (\\(s_B*\\)), the informed upvote probability of \\(A\\) needs to take all information below it in the tree into account. To know the informed probability of \\(A\\), we need to know the informed probabilities of all the replies to \\(A\\) as well, and for each of those, we need the informed probabilities of their replies too, and so on. To calculate all of those probabilities, the algorithm recurses through the discussion tree, starting at the top note. For leaf nodes, the informed upvote probability is equal to the uninformed upvote probability because leaf nodes have no replies which means reaching a leaf node is the stopping criterion for the recursion. The outcome of the algorithm are all the informed probabilities of the replies in a given discussion tree. We can compare them with the uninformed probabilities to find out which replies are most likely to change users’ votes. "],["cognitive-dissonance.html", "8 Cognitive Dissonance 8.1 Key Concepts from Information Theory 8.2 Surprisal as a Measure of Error 8.3 Total Cross Entropy 8.4 Total Relative Entropy = Cognitive Dissonance 8.5 Detailed Example 8.6 Discussion", " 8 Cognitive Dissonance As described in the Global Brain Overview, the goal of the global brain algorithm is to focus users’ attention on posts that reduce cognitive dissonance – difference of belief that only exist because people have been exposed to different information. When a note on a post changes the probability that users upvote the post, then there is cognitive dissonance in proportion to the number of people who voted on the post without being shown the note. Information theory lets us easily quantify this cognitive dissonance using the concept of entropy. 8.1 Key Concepts from Information Theory Here’s a quick summary of the relevant concepts of information theory: surprisal: how surprised I am when I learn that the value of X is x: \\[Suprisal(x) = -{\\lg P(X=x)}\\] entropy: how surprised I expect to be: \\[ \\begin{aligned} H(P) &amp;= 𝔼 Suprisal(X) \\\\ &amp;= 𝔼 -{\\lg P(X)} \\\\ &amp;= ∑_x P(X=x) × -{\\lg P(X=x)} \\\\ \\end{aligned} \\] cross-entropy: how surprised I expect Bob to be (if Bob’s beliefs are \\(Q\\) instead of \\(P\\)): \\[ \\begin{aligned} H(P,Q) &amp;= 𝔼 Suprisal_Q(X) \\\\ &amp;= 𝔼 -{\\lg Q(X)} \\\\ &amp;= ∑_x P(X=x) × -{\\lg Q(X=x)} \\end{aligned} \\] relative entropy or KL divergence: how much more surprised I expect Bob to be than me: \\[ \\begin{aligned} D_{KL}(P || Q) &amp;= H(P,Q) - H(P) \\\\ &amp;= ∑_x P(X=x) × {\\lg \\frac{P(X=x)}{Q(X=x)}} \\end{aligned} \\] When dealing with binary variables then these formulas can be written as: entropy: \\[ H(p) = - p × {\\lg p} - (1-p) × {\\lg (1-p)} \\] cross-entropy: \\[ H(p,q) = - p × -{\\lg q} - (1-p) × {\\lg (1-q)} \\] relative entropy or KL-divergence: \\[ D_{KL}(p||q) = - p × {\\lg \\frac{p}{q}} - (1-p) × {\\lg \\frac{1-p}{1-q}} \\] 8.2 Surprisal as a Measure of Error Surprisal can be thought of, for our purposes, as a measure of the “error” of a prediction. If we predict that something has a 1% chance of happening, and it happens, surprisal is \\(-{\\lg .01} = 6.64~bits\\), whereas if we predicted it had a 99% chance of happening, surprisal is only \\(-{\\lg .99} = 0.14~bits\\), which is much smaller. If we thought there was a 50/50 chance, surprisal is \\(-{\\lg .5} = 1~bit\\). Suppose we predict that the probability of a user upvoting a post is \\(p\\). Then suppose there are actually \\(upvotes\\) upvotes and \\(downvotes\\) downvotes. What is the total error of our prediction? Every time there is an upvote, surprisal is \\(-{\\lg p}\\). The probability of a downvote is just \\(1-p\\), so whenever there is a downvote surprisal is \\(-{\\lg (1-p)}\\). So our total error is: \\[ upvotes × -{\\lg p} + downvotes × -{\\lg (1-p)} \\] Since \\(upvotes ≈ votesTotal×p\\), and \\(downvotes ≈ votesTotal×(1-p)\\) our total error is approximately: \\[ \\begin{aligned} &amp; votesTotal × p × -{\\lg p} + votesTotal × (1-p) × -{\\lg (1-p)} \\\\ &amp; = votesTotal × H(p) \\end{aligned} \\] In other words, our error is roughly entropy times the number of votes. That’s why entropy is often described as the average surprisal, or expected value of surprisal – it is the surprisal per vote. However For our purposes, we will be defining cognitive dissonance in terms of total surprisal – specifically, as total relative entropy – which we will define below. But first we need to discuss the closely concept of cross entropy. 8.3 Total Cross Entropy Let’s say Alice estimates the probability of an upvote to be \\(q\\), but Bob thinks the probability of an upvote is \\(p\\). So Bob’s measure of Alice’s error will different from Alice’s measure of her own error! Bob’s measure of Alice’s error is: \\[ \\begin{aligned} &amp;upvotes × -{\\lg q} + downvotes × -{\\lg (1-q)} \\\\ &amp;≈ votesTotal×p × -{\\lg q} + votesTotal×(1-p) × -{\\lg (1-q)} \\\\ &amp;= votesTotal×( p×-{\\lg q} + (1-p)×-{\\lg (1-q)} ) \\\\ &amp;= votesTotal×H(p,q) \\\\ \\end{aligned} \\] \\(H(p,q)\\) is the cross entropy between Bob and Alices’s estimates: Bob’s estimate of Alice’s average error. In our case, Alice represents the average uninformed user (users who haven’t seen the note on a post), and Bob represents the informed user. There are \\(votesTotal\\) “Alices”, or uninformed users. Bob expects the total error of all the uninformed users to be \\(votesTotal×H(p,q)\\). \\(H(p,q)\\) will always be greater than \\(H(p)\\) if \\(p≠q\\). That is, Bob’s expects Alice’s error to be greater than his own error as long as his beliefs differ from Alice’s. This makes sense: Bob obviously thinks his beliefs are more accurate than Alice’s. If he didn’t he would update his beliefs to be equal to Alice’s. As more “Alice”’s are exposed to the note, and consequently the average Alice’s beliefs \\(q\\) approach Bob’s beliefs \\(p\\), \\(H(p,q)\\) will decrease, until \\(H(p,q)\\) = \\(H(p,p)\\) = \\(H(p)\\). 8.4 Total Relative Entropy = Cognitive Dissonance The difference between \\(H(p,q)\\) and \\(H(p)\\) is relative entropy, also known as the Kullback–Leibler divergence or KL-divergence. It can be thought of as “how much more surprised Bob expects Alice to be than himself”. Since \\(H(p,q) &gt;= H(p)\\), relative entropy is never negative, regardless of whether \\(p\\) is greater or less than \\(q\\). If we take Bob’s measure of Alices’s total error, minus his measure of his own total error, we get the total relative entropy: how much more error Bob expects for Alice than he would expect if Alice knew more. The total relative entropy is our measure of cognitive dissonance. \\[ \\begin{aligned} cognitiveDissonance &amp;= votesTotal × H(p,q) - votesTotal × H(p) \\\\ &amp;= votesTotal × ( H(p,q) - H(p) ) \\\\ &amp;= votesTotal × D_{KL}(p || q) \\end{aligned} \\] 8.5 Detailed Example Suppose a post without a note is given receives 900 upvotes and 100 downvotes. But a certain note, when shown along with the post, reduces the upvote probability to 20%. The relative entropy is \\[ \\begin{aligned} cognitiveDissonance &amp;= votesTotal × D_{KL}(p || q) \\\\ &amp;= votesTotal × ( p × {\\lg \\frac{p}{q}} + (1-p) × {\\lg \\frac{1-p}{1-q}} ) \\\\ &amp;= ( .2 × lg~\\frac{.2}{.9} + .8 × lg~\\frac{.8}{.1} ) \\\\ &amp;= 1.966~bits \\end{aligned} \\] Total cognitive dissonance is thus: \\[ \\begin{aligned} cognitiveDissonance &amp;= votesTotal × D_{KL}(p || q) \\\\ &amp;= votesTotal × ( p × {\\lg \\frac{p}{q}} + (1-p) × {\\lg \\frac{1-p}{1-q}} ) \\\\ &amp;= 1000 × 1.966 bits &amp;= 1966~bits \\end{aligned} \\] We can reduce cognitive dissonance by bring the note to the attention of users who didn’t see it before they vote. Some fraction of these users will change their vote after seeing the note. But some users are unlikely to actually change their vote, even if exposed to the note many times. Thus as we expose more and more of these users to the vote, the upvoteProbability for these users will approach $$q but never reach it. We may be able to estimate some global fraction of users who change/don’t change their vote, and use this to decide on which notes to direct attention to maximize the expected rate of reduction of cognitive dissonance, which is the overall goal of the global brain algorithm. See the next document on information value. 8.6 Discussion 8.6.1 Parallel to Machine Learning Cross entropy is commonly used as the cost function in many machine learning algorithms. A neural network for example takes an input with labels (e.g. images of cats and dogs) and outputs an estimated probability (e.g. that the image is a cat). The cost quantifies the error of these probability estimates with respect to the correct labels, and the neural network is trained by minimizing the cost function. If \\(ŷ_i\\) is the machine’s predicted probability for training example \\(i\\), and \\(y_i\\) is the correct output (1 or 0), then the total cross entropy cost is: \\[ \\sum_i H(y_i, ŷ) = \\sum_i y_i × -{\\lg ŷ_i} + (1-y_i) × -{\\lg(1 - ŷ_i)} \\] In our case, if we say that \\(y_i\\) are users votes, and \\(ŷ_i\\) is always equal to the uninformed users prediction \\(q\\), then our cost function is identical to the cost function used when training a neural network: \\[ \\begin{aligned} \\sum_i H(y_i, q) &amp;= \\sum_i y_i × -{\\lg ŷ_i} + (1-y_i) × -{\\lg(1 - ŷ_i)} \\\\ &amp;= \\sum_i y_i × -{\\lg q} + (1-y_i) × -{\\lg(1 - q)} \\\\ &amp;= upvotes × -{\\lg q} + downvotes × -{\\lg(1 - q)} \\\\ &amp;≈ votesTotal×H(p,q) \\\\ \\end{aligned} \\] So both neural networks and the global brain “learn” by reducing cross entropy. The difference is that the global brain reduces entropy not by learning to make better predictions, but by in a sense teaching users to make better predictions of how a fully-informed user would vote. 8.6.2 Subtle Point 1 Our measure of cognitive dissonance is relative entropy, not cross entropy. But minimizing relative entropy is the same as minimizing cross entropy, since relative entropy \\(H(P,Q) - H(P)\\) is just cross entropy minus a constant \\(H(P)\\) (\\(H(P)\\) is a constant since there’s nothing we can to do change the true probability \\(P\\)). 8.6.3 Subtle Point 2 Note that cross-entropy in our case is a measure of the total surprise of uninformed users at the hypothetical future votes of informed users. That is to say, it is: \\[ \\begin{aligned} &amp;votesTotal×p × -{\\lg q} \\\\ &amp;+ votesTotal×(1-p) × -{\\lg (1-q)} ) \\\\ &amp;≈ hypotheticalInformedUpvotes × -{\\lg q} \\\\ &amp;+ hypotheticalInformedDownvotes × -{\\lg (1-q)} ) \\\\ &amp;= votesTotal×H(p,q) \\end{aligned} \\] And not \\[ \\begin{aligned} &amp; votesTotal×q × -{\\lg q} \\\\ &amp;+ votesTotal×(1-q) × -{\\lg (1-q)} ) \\\\ &amp;≈ actualUpvotes × -{\\lg q} \\\\ &amp;+ actualDownvotes × -{\\lg (1-q)} ) \\\\ &amp;= votesTotal×H(q) \\end{aligned} \\] This is potentially confusing (it caused me a great deal of confusion initially) because we are measuring the error of Alice’s estimated probability \\(q\\) with respect to hypothetical events that have have not yet actually occurred. But shouldn’t we be measuring the error of the votes that have occurred? No, because surprisal is a measure of the error of a probability estimate, no the error of the event. The events are “what actually happens” or, in case of uncertainty, “what we think will actually happen” given our best current estimate \\(p\\). Measuring error against actual vote events just gives us \\(votesTotal*H(q)\\), which doesn’t tell us how much actual votes differ from what they should be if users were more informed. \\(votesTotal×H(p,q)\\) makes the most sense as a measure of the tension between the current state of user’s beliefs, and what that state should be, which is a hypothetical future where all users have changed their upvotes because of the information in the note, and thus \\(actualUpvotes = hypotheticalInformedUpvotes\\) and \\(actualDownvotes = hypotheticalInformedDownvotes\\). "],["information-value.html", "9 Information Value 9.1 The Information Value of a Vote 9.2 Information Value of Changed Votes 9.3 Information Value of New Votes 9.4 What Does a Vote Mean? 9.5 Example 1: A Storm in Madrid 9.6 Example 2: A Typhoon in Oslo 9.7 Desired Properties of Information Value Formula 9.8 Upvote-Only Relative Entropy 9.9 Example Charts", " 9 Information Value The goal of the global brain algorithm is to focus users’ attention on posts that reduce cognitive dissonance. Our formula for measuring cognitive dissonance is described here. In information theory, information is the reduction of entropy. Since we define cognitive dissonance as entropy, reducing cognitive dissonance can also be seen as gaining information. Thus the goal of the global brain algorithm is to directs attention to notes with the greatest potential information gain. In the section on cognitive dissonance, we explain how relative entropy is reduced as users change their votes, which means that information is gained when users change beliefs. But how much information is gained when users upvote or downvote a new post they have not previously voted on? We don’t know how much that post changed users beliefs, because we don’t know what their beliefs were before they saw the post. Without two probability distributions to compare, we can’t calculate information value in terms of reduction of entropy. Yet obviously new posts do convey information – something users didn’t know or hadn’t seen before, or they wouldn’t upvote the post. By making some conservative estimates about users’ prior beliefs, we can quantify the information gain for new posts and derive a single metric for estimating the information value of a vote, whether that vote is a new upvote (new information learned by users), or a changed vote (decrease in cognitive dissonance). In this article we will introduce the formula and the reasoning behind it. 9.1 The Information Value of a Vote First, here is a summary of the information value formula: new votes: \\[ votesTotal × p × (1 + {\\lg p}) \\] changed votes: \\[ \\begin{aligned} votesTotal × ( p × {\\lg \\frac{q_1}{q}} + (1-p) × {\\lg \\frac{1-q_1}{1-q}} ) \\end{aligned} \\] where: p is the informed probability q is the uninformed probability q1 is the probability after \\(deltaUpvotes\\) votes where changed: \\((upvotes + deltaUpvotes)/votesTotal\\) 9.2 Information Value of Changed Votes Whenever the global brain learns that a note has change the probability of an upvote on a post, \\(p\\) becomes different from \\(q\\) and therefore cognitive dissonance is created. But as users are shown the note and change their votes so that \\(q\\) once again approaches \\(p\\), cognitive dissonance is reduced. The information value from moving from \\(q\\) to \\(q_1\\) is just the reduction in cognitive dissonance, or reduction in total relative entropy. \\[ \\begin{aligned} votesTotal × D_{KL}(p || q) - votesTotal × D_{KL}(p || q_1) \\end{aligned} \\] This equation simplifies to: \\[ \\begin{aligned} &amp; votesTotal × D_{KL}(p || q) - D_{KL}(p || q_1) ) \\\\ &amp;= votesTotal × ( p × {\\lg \\frac{p}{q}} + (1-p) × {\\lg \\frac{1-p}{1-q}} ) \\\\ &amp;- votesTotal × ( p × {\\lg \\frac{p}{q_1}} + (1-p) × {\\lg \\frac{1-p}{1-q_1}} ) \\\\ &amp;= votesTotal × p × {\\lg \\frac{q_1}{q}} + (1-p) × {\\lg \\frac{1-q_1}{1-q}} \\end{aligned} \\] 9.3 Information Value of New Votes In the case of new votes, measuring the change in entropy is harder because we don’t know what users’ beliefs were before they voted. Yet clearly new posts provide information, otherwise users would not upvote them. The upvotes tell us that the post has changed beliefs in some way. They caused users to believe things they didn’t believe before. TODO fix here: To create an information value formula that makes sense for new upvotes, we need recognize that new upvotes mean something different from changed upvotes. 9.4 What Does a Vote Mean? We assume that an upvotes reflect the intent for a post to receive more attention. Further, we assume that the intent of the community is to find information that is both: informative, and relevant A post is informative if it causes a user to hold a belief, or mental state, they didn’t hold before. Simple statements of facts are informative if the user trusts the source. There’s going to be a big storm in Madrid tomorrow is likely informative. A joke can also be informative if the user hadn’t heard the joke before, because it causes a change in belief or hidden mental state (here is a funny joke!). Bigfoot exists is likely not informative, because people are unlikely to believe bigfoot exists just because somebody on the internet said so. Therefore, we assume that people do not upvote posts that are not informative. On the other hand, people downvote posts when they think they are not informative or not relevant. A downvote means that a post did not change beliefs (or if it did, that belief was irrelevant). If somebody downvotes there’s going to be a big storm in Madrid tomorrow it’s because they think it’s irrelevant (e.g. if it is posted in the wrong forum) or uncertain. But they believe this despite seeing the post, not because of it. The post itself did not change their beliefs. So there is an asymmetry between upvotes and downvotes. If the information is in fact false, people who downvoted weren’t harmed, because they knew it was false an their beliefs did not change (yes their time was wasted, but let’s ignore wasted time and focus on the idea of cognitive dissonance). But those who upvoted it changed their beliefs and these beliefs are false. So upvotes on false information indicate harm was done. Users were misinformed, and misinformation has negative information value. 9.5 Example 1: A Storm in Madrid For example, suppose I have not yet seen post A, telling me about a big storm forecast for Madrid tomorrow, posted in a Spain travel forum. Users who have not yet heard about the storm do not believe there will be a big storm tomorrow. They are probably not absolutely certain about the belief, but their prior beliefs before seeing the post, if expressed as a probability, are probably quite small. Suppose that most users who see this post upvote it, but almost nobody downvotes it. This means that a lot of people didn’t know about the storm before they saw the post: a lot of information was gained for these users. It also means there is a lot of potential information value for users who have not yet seen the post. So a lot of upvotes, with relatively few downvotes, represent an information value proportional to the number of upvotes. But as downvotes increase, uncertainty increases rapidly. By the time the number of downvotes exceeds the number of upvotes, the post is deemed by the community as more likely to be misinformation than information. 9.6 Example 2: A Typhoon in Oslo Now consider a post that receives a very large number of downvotes, and only 1 upvote. For example a post falsely claiming that a category 5 typhoon will hit Oslo tomorrow (a geographical impossibility). How much information was gained, or lost? Here the upvote tells us somebody actually believed the post (still assuming good faith voting). So somebody now believes something they didn’t believe before. And what’s more, what they believe is probably not true and relevant, in the judgment of the community. So the increase in the number of users who believe something that is probably not true and relevant actually increases entropy. Now suppose instead of only 1 upvote, there were 10 upvotes. Now 10x as much entropy was created, because 10 users were misinformed, not just 1. So a post with a very low upvote rate indicates an information loss (negative information value) proportional to the number of upvotes. 9.7 Desired Properties of Information Value Formula So for new information, the information value formula should have the following properties: Many upvotes, no downvotes: informationValue ≈ upvotes Many downvotes, no upvotes: informationValue ≈ 0 Many downvotes, small number of upvotes: informationValue ≈ -upvotes Equal number of upvotes and downvotes: informationValue ≈ 0 9.8 Upvote-Only Relative Entropy These properties are all met by the formula: \\[ \\begin{aligned} upvotes × (1 + {\\lg p}) \\end{aligned} \\] This formula is based on two assumptions: The average users prior beliefs before upvoting is \\(q=0.5\\) We only count the information value of upvotes We don’t know what users beliefs were before they voted, but we can make a conservative estimates about those prior beliefs using the maximum entropy principle. The probability with maximum entropy (for binary outcomes such as upvote/downvote) is \\(q=0.5\\): which has an entropy of 1 bit. Next, going back to the fundamental concept of surprisal from information theory, which we introduced when developing the formula for cognitive dissonance, we can calculate the total suprisal of upvotes as \\[ upvotes × -{\\lg p} \\] And our most conservative estimate of the total surprisal before users saw the post is \\[ upvotes × -{\\lg 0.5} \\] The difference is \\[ \\begin{aligned} &amp;upvotes × -{\\lg 0.5} - (upvotes × -{\\lg p}) \\\\ &amp;= upvotes × {\\lg \\frac{p}{0.5}} \\end{aligned} \\] This is basically the formula for relative entropy, \\(upvotes × D_{KL}(p, 0.5)\\), but “cut in half”: counting only the upvotes. Since \\(-{\\lg 0.5} = 1\\), we can further simplify this to \\[ \\begin{aligned} upvotes × (1 + {\\lg p}) \\end{aligned} \\] One way of looking at this formula is to think of each upvote as representing 1 potentially-uncertain bit of information. If \\(p\\) is 100%, then there is no uncertainty, and each upvote is worth 1 bit of information. But if \\(p\\) is, say, .25, then there are \\(-{\\lg .25} = 2\\) bits of uncertainty about the information. For each user who accepted the information, uncertainty was reduced by 1 bit, but another \\(-{\\lg .25} = 2\\) bits of uncertainty about the accuracy of the information was created, for a net loss of 1 bit. 9.9 Example Charts The chart below illustrates how the information value of a post with a large number of upvotes falls as the number of downvotes increases (e.g. the Storm in Madrid example, but as doubts about the accuracy of the forecast start to increase). The chart below illustrates how the information value of a post with a large number of downvotes becomes increasingly negative as the number of upvotes increases, but then begins to rise as the information becomes less uncertain. "],["informtaion-theory-primer.html", "A Primer on Information Theory I A.1 Intuitions about Key Concepts A.2 Surprisal A.3 Entropy", " A Primer on Information Theory I We will introduce some key concepts of information theory that are prerequisites for understanding the Global Brain algorithm. Note that for all the concepts introduced in the following chapters, we will only discuss the case of discrete probability distributions because that’s enough for our purposes. All of the concepts generalize to the continuous case though and should you be inclined to study those too, this introduction will give you a good start all the same. A.1 Intuitions about Key Concepts There are four key concepts in information theory that are important to have a good grasp on in order to understand the Global Brain algorithm: surprisal, entropy, cross-entropy, and KL divergence (or relative entropy). We will cover all of these concepts in detail in the following chapters, but for now, let’s just build an intuition about what each of those measures mean semantically. Consider Alice and Bob, two friends who argue about a random variable \\(X\\). Alice’s beliefs about \\(X\\) are distributed according to \\(P\\) and Bob’s beliefs are distributed according to \\(Q\\). Then from Alice’s perspective, the concepts listed above mean the following: surprisal: how surprised Alice is when she learns that the value of X is x entropy: how surprised Alice expects to be about the outcome of X cross-entropy: how surprised Alice expects Bob to be (assuming that Bob believes \\(Q\\)) relative entropy or KL divergence: how much more surprised Alice expects Bob to be than her A.2 Surprisal In the Bayesian way of thinking, the concept of probability gives us a way to talk about uncertainty. Roughly speaking, we quantify our beliefs about any outcome of a random variable by assigning each outcome a weight (the probability we assign to that event), making sure that all weights add up to \\(1\\). As the notion of probability is so fundamental to statistics and probability theory, it’s easy to overlook that this is not the only way of thinking about uncertainty. A different approach that is fundamental to information theory is the notion of surprisal. Surprisal expresses our uncertainty about an event by quantifying how surprised we would be, should the event occur. The notion of surprisal is consistent with a Bayesian vocabulary because it acknowledges the subjectivity of belief and we can derive it from probability. First of all, we want surprisal and probability to be inversely proportional to one another: The more likely and event is to occur, the less surprised we will be about it. A naive way of expressing this would be to just take the inverse of the probability \\(p\\): \\[ \\frac{1}{p} \\] So far so good, but this is not the actual formula for surprisal because this metric has an unsatisfying property: If something is certain to happen, i.e., the probability is \\(1\\), our surprisal will be \\(\\frac{1}{p} = \\frac{1}{1} = 1\\). But if it is certain to happen, we would expect our surprisal to be \\(0\\). There is a function that we can use to scale the inverse probability, so that we can satisfy this requirement: the logarithm. The logarithm of \\(1\\) for any base \\(b\\) is \\(0\\) which is exactly the property we require. Thus, if we simply take the logarithm of the inverse of probability, we can scale our formula to have the required properties. This is the actual formula for surprisal: \\[ I(x = X) = log_b(\\frac{1}{P(x = X)}) = -log_b(p) \\] We use the symbol \\(I\\) to denote surprisal because it is also commonly referred to as information content3. What about the Base of the Logarithm? You may have noticed that we have left the base \\(b\\) of the logarithm in the formula for surprisal open. This is because the base of the logarithm defines the unit of information at which we are measuring surprisal. The most common one (and the one we use in our approaches) is base \\(2\\). With base \\(2\\), we express information in bits (= “binary digits”). If you choose base \\(10\\), the unit is a decimal digit (or Hartley) and base \\(3\\) gives you a trit. Thus, with different values for \\(b\\), we can change the unit of information at which we are expressing our surprise. Here is what the relationship between probability and surprisal looks like graphically with different bases \\(b\\): The function is strictly decreasing so surprisal inversely proportional to probability. Note that surprisal is undefined for a probability of \\(0\\). This is fine because we have no need to express our surprise about an event that will never occur. A.3 Entropy Entropy is the expected value of surprisal. This may sound odd at first. A measure that quantifies how surprised I expect to be sounds like an oxymoron. But it does make sense. First of all, with probability, we encode our uncertainty about outcomes of a random variable. We are using all the information we have, but we know that we don’t have all the information. Thus, we know that we will be surprised about the outcome in some way because the outcome is not certain. Entropy gives us a way to quantify how surprised we expect to be about the outcome. Entropy is given by4: \\[ H(X) := -\\sum_{x \\in X}{p(x)log(p(x))} \\] Useful resources on surprisal and entropy StatQuest on surprisal and entropy (video) Intuitively understanding the Shannon Entropy (video) Wikipedia on information content Other terms for surprisal that you might come across are self-information and Shannon information.↩︎ You may ask yourself: Why is \\(H\\) the formula symbol for a metric called entropy? That’s because Shannon entropy is based on Boltzmann’s H-theorem in statistical thermodynamics. It is contested why Boltzmann called entropy \\(H\\), but the common conjecture is that he actually meant the greek letter Eta (\\(H\\)) which would make a lot more sense. There have actually been typographical analyses of his handwriting to determine whether he meant Eta, not the letter \\(H\\)!↩︎ "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

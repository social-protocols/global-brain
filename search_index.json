[["index.html", "Introduction to the Global Brain Chapter 1 About", " Introduction to the Global Brain Last updated: 2023-10-18 Chapter 1 About This document contains write-ups and explanations on the Global Brain algorithm developed by the social protocols organization. A proof of concept of this algorithm is being developed in the [ùïê platform] (https://github.com/social-protocols/Y). ùïê has not been launched yet and is under rapid development, so this is a living document and the information in it may become outdated fast. Nonetheless, we try to maintain up-to-date explanations of our ideas here. We will introduce the building blocks of our algorithms; simple, but important concepts which make up the foundation of the components from which the ùïê platform is built. We are grateful for feedback. Do not hesitate to send a [mail] (mailto:mail@social-protocols.org). "],["global-brain-overview.html", "Chapter 2 Global Brain Overview 2.1 Informed Votes 2.2 The Causal Model 2.3 Distributed Reasoning 2.4 Informal Argument Model 2.5 Example 2.6 Reason and Argument 2.7 Checking Misinformation 2.8 Marketplace of Ideas 2.9 Defining Cognitive Dissonance 2.10 Cognitive Dissonance as Relative Entropy 2.11 The Causal Model", " Chapter 2 Global Brain Overview Falsehood flies, and truth comes limping after it, so that when men come to be undeceived, it is too late‚Ä¶ ‚Äì Jonathan Swift The Global Brain algorithm converts a social network into a distributed brain, where groups of individuals act like neurons that process information and pass it on to others, resulting in a whole that is more intelligent than the individual human parts. The algorithm works by identifying information that is likely to change minds, and modeling how those changes propagate through the network. It then uses this information to focus attention on posts that reduce ‚Äúcognitive dissonance‚Äù ‚Äì difference of opinion due to people being exposed to different information. The result is a social protocol that drives productive conversations, maximizes the flow of useful and reliable information, reduces the flow of misinformation, and increases human alignment. 2.1 Informed Votes The basic computational unit in the Global Brain is the informed vote. Before users vote on a post, the UI shows them a note (any reply to that post), and then records their vote given they were shown that note before voting. If the informed vote is different from the uninformed vote, it means the note caused changes in votes. We call notes that do change behavior informative. 2.2 The Causal Model So every reply to every post becomes effectively an AB test that tells us how exposure to a note affects votes on a post. And one thing that makes properly-run AB tests very powerful is that they establish causality. They tell us not just that upvotes on the note and post are correlated, but that the note actually causes changes to votes on the post. Establishing these causal links allows us to model the Global Brain as a causal Bayesian network. The causal links between notes and posts are the synapses of the Global Brain. They allow us to predict how the average user would vote if they were exposed to some combination of notes and posts, by predicting how exposure to those posts will influence upvotes on other posts, and how that influences upvotes on other posts, and so on. 2.3 Distributed Reasoning By simulating how beliefs propagate through the network, the global brain engages in a form of reasoning, loosely defined. Each neuron in the global brain reasons ‚Äì or processes information ‚Äì using the same inferences the average user makes. But unlike the average user, the Global Brain can process all information in the system in a valid Bayesian manner. So in a way the global simulates an average user who has unlimited time, memory, and processing capacity. We can then query this model to estimate how the Global Brain would vote if it was exposed to all information ‚Äì all posts ever made. We can also use the model to identify where to direct users‚Äô attention in order to reduce cognitive dissonance in the network. 2.4 Informal Argument Model The Global Brain does not require a formal model of belief. It doesn‚Äôt need posts to be structured as formal propositions or claims. It doesn‚Äôt need to know if users agree or disagree with posts. The only assumptions are that 1) a vote reflects a users intent to give a post more/less attention (see The Law of Attention) and 2) this intent is caused by underlying beliefs that we cannot observe (latent variables ‚Äì see section below). But by making some reasonable assumptions about the causal relationship between beliefs and votes, then by watching how exposure to notes causes changes to votes, we can model these underlying beliefs and predict how exposure to a note will cause changes to these beliefs, which will cause changes to other beliefs, and so on. We described our model in more detail below. 2.5 Example For example, suppose post A is a video showing the aftermath of a recent earthquake. This post goes viral, but then somebody posts a note B on the video which says ‚ÄúThis video is actually from an earthquake in 2004. Here is a link to the original video.‚Äù. Suppose that people who are shown note B along with the video in post A are less unlikely to upvote A. This will often be the case, as many people probably upvoted out of simple ignorance, not malicious intent to spread misinformation. The global brain will conclude that: B is informative A should receive less attention, because informed users are unlikely to upvote it B should receive more attention from users who have already seen or upvoted A, in order to reduce cognitive dissonance Now, suppose that it turns out the information in note B is actually wrong. For example, somebody may reply to B with a subnote C saying ‚ÄúNo, the video in that link is completely different! This video is recent footage from CNN. Here‚Äôs a link.‚Äù The global brain can actually look at how C affects users‚Äô votes on both B, to estimate how a fully informed user would have voted on A. If post B links different groups of users in different conversations ‚Äì for example if B is ‚Äúreposted‚Äù or cross-posted to more than one forum ‚Äì then the global brain can observe how C affects B in one forum, and how B affects A in another forum, and ‚Äúlink‚Äù the two forums, predicting the probability that a fully-informed user woudl upvote A after seeing subnote C, even if no user whote voted on A actually saw subnote C. 2.6 Reason and Argument A key assumption we make is that more information leads to better judgments. Even though people can post false and misleading information, people can respond by explaining why this information is false or misleading. Even though people can promote hateful or harmful ideas, people can respond with reasons why these these ideas are wrong. By identifying information that changes minds, driving attention to that information, and then identifying responses that change minds back, the system drives informative conversations. And although this isn‚Äôt guaranteed to lead to the truth, it will at least result in more informed users. And it will also help identify the most informed opinions: what posts would still be upvoted by users who were exposed to all the strongest information on arguments on all sides. In the example above, once the model learns that B is changing people‚Äôs votes on A, it directs more attention to B, resulting in fewer upvotes for A. We can think of B as an argument against A, or a reason not to upvote A. The ‚Äúreason‚Äù doesn‚Äôt need to take any particular logical or rhetorical form. It is a reason by virtue of being the literal reason for ‚Äì the observed cause of ‚Äì changes to votes. Unfortunately, in the example above, B was not true. But fortunately, C is a convincing reason to believe that B is not true. C actually causes people not to change their vote on A, in spite of being exposed to B. B and C cancel each other out, so to speak. So the global brain concludes that it is no longer helpful to direct attention to B, and that it should continue to direct attention to post A. 2.7 Checking Misinformation Misinformation is harmful in social networks only when it is unchecked. Lies are only harmful if people don‚Äôt see the responses to the lies. The engagement-based algorithms in today‚Äôs social networks encourage the unchecked spread of rumors (with the exception of X‚Äôs Community Notes), regardless of how well they are supported, whereas the Global Brain algorithm discourages the spread of information that people are unlikely to upvote if they were fully informed, and encourages the spread of information that might check the spread of a false information that is already spreading. And yet the purpose of the Global Brain algorithm is NOT to tell people what is true. It is simply to direct people‚Äôs attention to information that may change their votes. But why do we want to change votes? An algorithm that tries to change minds sounds sinister. Propaganda changes how people vote. But propaganda works by selectively exposing users to information with the goal of changing opinions about specific things, all the while actively omitting contrary information. It is one sided and dishonest. The Global Brain, on the other hand, has no agenda other than driving productive conversation and reducing cognitive dissonance. 2.8 Marketplace of Ideas So the Global Brain algorithm drives a fair, unbiased process of weighing all the arguments that anyone cares to make. As long as there is sufficient intellectual diversity among recipients, the result is an adversarial process, where all the relevant information (the posts that change minds either way) is exposed and processed (the marketplace of ideas). 2.9 Defining Cognitive Dissonance The Global Brain only requires a small number of people to actually change their minds in order to learn how beliefs affect other beliefs and construct the causal Bayesian network. So the predictions of the model may differ significantly from the beliefs of the majority. For example, suppose 1000 users voted on a post, and most votes were upvotes. Then suppose that among a subset of 30 users who saw some very informative note, the probability of upvoting the post dropped to close to zero. We can thus estimate that if all 1000 users saw that note, they would not have upvoted the post. Even though in actuality most users did upvote it. In such situations, we say there is a large amount of cognitive dissonance in the network. Cognitive dissonance arises whenever people‚Äôs upvotes differ from what their upvotes would be if they were exposed to all the relevant information. Or in other words, when peoples votes differ from the predictions of the causal model. The goal of the Global Brain algorithm is minimizing cognitive dissonance. It does this by exposing users to the informative note, reducing the inconsistency between what people upvote and what they would upvote if they were more informed. Reducing cognitive dissonance brings individual participants into greater alignment, reducing differences of opinion due to differences in information. It cannot of course eliminate differences of opinion, because opinion can be subjective, because people have different priors and different ways of processing information. But it can identify situations where differences are due to ignorance of readily available information: when one group of people believe something that another group doesn‚Äôt only because they haven‚Äôt seen the same posts. The result is a network that learns. The model learns as new posts are submitted to the network. And the human participants in the network also learn as they are exposed to posts that bring their mental models into greater alignment. 2.10 Cognitive Dissonance as Relative Entropy We can measure cognitive dissonance as the relative entropy (or KL divergence) between users‚Äô actual beliefs and their hypothetical fully-informed beliefs (the beliefs they would have if exposed to all posts). Relative entropy is a way of measuring the distance, or error, between two probability distributions. It is similar to the measure used as the loss function when training a neural network, and in this sense, the global brain can be seen to ‚Äúlearn‚Äù in a sense similar to how a neural network learns. Optimizing for reducing cross entropy is sufficient to drive productive conversations, because it directs attention exactly where it needs to be directed. Consider again the example above, where subnote C ‚Äúcancels out‚Äù the effect of note B on post A. This means that there is no difference between users beliefs about post A and their fully-informed beliefs, given they have seen B and C. So relative entropy is zero, and there is no value to be gained from directing more users‚Äô attention to note B. On the other hand, since some users have already seen B and had their minds changed, there is a difference between their probability of voting on post A and the fully-informed probability. So there is still cognitive dissonance among the subset of users who were exposed to B. Exposing that subset of users to post C, until the upvote percentage of that subset matches the overall upvote percentage, will result in minimal cross-entropy. The theory and math for calculating cross-entropy and ranking posts based on their potential to reduce cross-entropy is developed in Cognitive Dissonance and Information Rate. 2.11 The Causal Model The global brain requires modeling users‚Äô beliefs as a causal Bayesian network. But making this link is tricky, because we don‚Äôt actually know what users believe, only how they vote. Suppose we know how much exposure to post B changed the probability that users will upvote post A, and how much exposure to post C changes the probability that a user will upvote post B. Can we predict how much exposure to post C will change the probability that a user will upvote post A? We can draw a causal graph with our assumptions of how information causes changes in votes. We assume that votes are influenced by hidden variables, which are users‚Äô actual beliefs. For example, when somebody posts B, and it influences their vote on A, B must contain some information that users didn‚Äôt already have, and that gives them a reason to change their vote. If B did not contain new information, why would it change their behavior? Even a humorous comment can be modeled as information for our purposes. Suppose somebody responds to post A with a purely humorous comment B, and this causes more people to upvote A. Why would a joke cause more people to upvote A? Since we assume votes indicate intent to direct attention, it can only mean that the fact that joke B was funny caused people to believe that A should get more attention. Maybe it convinced them that A could be an amusing topic, for example. But we don‚Äôt need to have a theory of how people reason or what it is in people‚Äôs minds that cause them to change their behavior. It is sufficient to assert that there is some latent variable that links exposure to a post to voting behaviors. However, to understand this process it helps to think in purely Bayesian terms. So we say if something causes people to change votes, it is a reason. So we can say that these reasons are unobserved ‚Äúunderlying beliefs‚Äù in some proposition. Let‚Äôs say ùëç represents users‚Äô underlying belief that was directly changed by the post B (e.g.¬†B is a video of Bigfoot, which causes users to change their belief in ùëç=there is video evidence that Bigfoot exists). The belief in ùëç has a causal effect on votes on B. ùëç also has a causal effect on some other underlying belief ùëå (e.g.¬†ùëå=Bigfoot exists). Finally, the underlying belief ùëå has a causal effect on upvotes on post A (‚ÄúBigfoot is real, people!‚Äù) Then let‚Äôs use the term ùëÜùëè and ùëâ to represent the event that users were exposed to posts B and A respectively. And finally we‚Äôll use italic ùêµ and ùê¥ to represent upvotes on B and A respectively. So showing a user post B (ùëÜùëè) affects belief in ùëç which affects votes ùêµ, and so on. And belief ùëç also affects belief ùëå. So our causal graph looks like this. ùëÜùëè ‚Üí ùëç ‚Üí ùêµ ‚Üì ùëå ‚Üí ùê¥ Now, let‚Äôs assume that the number of upvotes ùê¥ or ùêµ is proportional to the number of people that believe ùëå or ùëç respectively. \\[ P(A = 1) \\propto P(Y = 1) \\] \\[ P(B = 1) \\propto P(Z = 1) \\] So we can kind of use ùê¥ and ùêµ as proxies for ùëå and ùëç. Now, suppose a user responds to post B with post C That video was generated by AI!. Seeing post C (ùëÜùëê) indirectly causes a change in the probability that users upvote A. And for the moment, assume that ùëÜùëê only affects ùëå through ùëç. So seeing post C affects the belief that ùëç=there is video evidence that bigfoot exists which affects the belief ùëå=bigfoot exists which affects ùê¥. Our causal graph looks like this: ùëÜùëê ‚Üí ùê∂ ‚Üì ùëÜùëè ‚Üí ùëç ‚Üí ùêµ ‚Üì ùëå ‚Üí ùê¥ Now, what can we do with this causal graph? Well, assuming the causal assumptions in the graph are true, we can deduce how ùëÜùëê will affect ùëå (through ùëç), without directly observing either ùëå or ùëç (the hidden states in users‚Äô minds), simply by observing how ùëÜùëê affects ùêµ and how ùëÜùëè affects ùê¥. This means that we can model long chains of ‚Äúreasoning‚Äù, and predict how one piece of information would propagate through the global brain, even if there is no individual who actually engages in that chain of reasoning. For example, post B (the video of bigfoot) might be cross-posted to a forum dedicated to identifying AI-generated videos. If comments in that forum convince users in that forum that the video was AI generated, then the global brain can deduce that some people in the cryptobiology forum would also be convinced. The groups of users in the two forum don‚Äôt have to overlap for information to flow between forums. This means that for very complex issues, where no individual can come close to being able to process all the information in the network, the global brain can essentially simulate a fully-informed individual by breaking the problem down into parts and propagating information between conversations using posts that are ‚Äúcross-posted‚Äù. The math for calculating belief propagation is described [in another document]. "],["rating-and-evaluating-content.html", "Chapter 3 Rating and Evaluating Content 3.1 Modeling Upvote Rates 3.2 Modeling Belief about the ‚ÄúTrue‚Äù Upvote Rate 3.3 Estimating the True Upvote Rate 3.4 The Bayesian Average", " Chapter 3 Rating and Evaluating Content The basic form of content users can provide on the Y platform is a post. In the following explanations, we visually represent a post by a rectangle with rounded corners like this. A post is the start of a conversation. Users can leave replies to posts which then form a conversation thread. Replies are posts themselves. For each post that has any replies, we try to find the reply that adds the most useful context to the post. We call this reply the post‚Äôs note (in reference to X‚Äôs Community Notes from which we took a lot of inspiration). Obviously, ‚Äúmost useful‚Äù is a very vague term and we will go further into our rationale what the most useful context is later on. Users on Y can rate content by adding an upvote or a downvote. There are two essential forms of content users can rate: posts and posts with a note. We call those units Voting Units. For each Voting Unit, we keep a tally of upvotes and downvotes. 3.1 Modeling Upvote Rates A naive way of using upvotes and downotes for rating a post would be to simply calculate the ratio between them: \\(upvotes:downvotes\\). Or expressed as the fraction of upvotes over all votes: \\[ \\frac{upvotes}{upvotes + downvotes} \\] We call this rate the upvote rate. Another way of thinking about the upvote rate is as an average vote. If we model the upvote rate as a random variable, we might encode upvotes with a value of \\(1\\) and downvotes with a value of \\(0\\). A sample post‚Äôs gathered votes at a certain point in time \\(t\\) might look like this: \\[ (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) \\] The post has gathered a total of 12 votes of which 3 are upvotes and 9 are downvotes. The upvote rate (i.e., the ‚Äúaverage‚Äù vote) for this sample is: \\[ \\rho = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{upvotes}{upvotes + downvotes} = \\frac{3}{3 + 9} = 0.25 \\] 3.2 Modeling Belief about the ‚ÄúTrue‚Äù Upvote Rate We assume that each post has a ‚Äútrue‚Äù upvote rate that we can only estimate by collecting data on it (i.e., collecting votes on a post). We model the true upvote rate as a random variable that follows a Beta distribution: \\[ upvoteRate \\sim \\beta(upvotes, downvotes) \\] We use this model because yet another way of thinking about the upvote rate is as the probability of a vote being an upvote and the Beta distribution is a suitable model for proportions or probabilities. The Beta distribution has two shape parameters \\(\\alpha\\) and \\(\\beta\\) which in our case are given by our upvote and downvote counts. To provide a more intuitive understanding of how this distribution models our upvote rate, here is an example of how it develops for an example post. We initialize the distribution with a prior of 1 upvote and 1 downvote. This is equivalent to a uniform distribution: We assign equal probability to any outcome, i.e., to any upvote rate. Now let‚Äôs use the previous example vote sample from above: \\[ (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) \\] Using the example above, we get the following distributions at different points in time: As you can see, the distribution updates and the probability mass becomes more concentrated around a smaller range of values which means we become more and more certain about our beliefs about the true upvote rate. If the post above would develop further with a similar trajectory, the following could be the outcome after 500 votes: We are now relatively certain that the true upvote rate is somewhere between around 0.2 and 0.3. 3.3 Estimating the True Upvote Rate Now that we can express our beliefs about the true upvote rate, how do we make a ‚Äúbest guess‚Äù at any given point in time? A naive solution would be to just take the sample upvote rate as introduced above: \\[ P(U) = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{upvotes}{upvotes + downvotes} \\] This solution is naive because it ignores an important fact: In the beginning, we have very little information (in fact no information) about the upvote rate of the post. Thus, each new arriving vote has an outsized effect on our estimate that just gets increasingly small over time. This would result in very erratic estimates in the beginning which would only smooth out over time. Here is how the cumulative mean develops for a random vote history with a true upvote rate of 0.6: If we were to use this metric to compare posts, getting a very high upvote rate estimate in the early stages of a post would essentially come down to luck. It might then temporarily fare overly well (or overly poorly) compared to other posts. We have to take into account prior information to avoid this. 3.4 The Bayesian Average The Bayesian Average uses a weighted prior estimate of the average to avoid the erratic shifts in the estimate when there is not a lot of data. It is calculated as follows: \\[ \\frac{Cm + \\sum_{i=1}^n x_i}{C + n} \\] \\(C\\) is a weight constant and \\(m\\) is our prior belief about the average. But what does this achieve in concrete terms? Let‚Äôs build up to this formula step by step using our example of estimating the true upvote rate of a post. First, remember that the sample upvote rate constitutes the ‚Äúplain‚Äù average. Let‚Äôs say our dataset \\(X\\) consists of \\(1\\)s and \\(0\\)s, where \\(1\\) denotes an upvote and \\(0\\) denotes a downvote. Then \\(\\sum_{i=1}^n{x_i}\\) is the number of upvotes (because downvotes are encoded as \\(0\\)s) and \\(n\\) is the total number of votes \\(upvotes + downvotes\\). The plain average is calculated as follows: \\[ \\frac{\\sum_{i=1}^n x_i}{n} \\] Or in concrete terms: \\[ \\frac{upvotes}{upvotes + downvotes} \\] The Bayesian average is thus: \\[ \\frac{C \\cdot m + upvotes}{C + upvotes + downvotes} \\] Now what does it mean that we add \\(C \\cdot m\\) to the nominator and \\(C\\) to the denominator? Practically, adding these terms to our formula means that we calculate the cumulative average as if we had collected \\(C\\) votes with an upvote rate of \\(m\\) before we collected the first vote on our post. If our prior belief about the average is 0.68 and we chose a weighting factor of 10, this would mean that we calculate the average as if we had previously collected 10 data points which amounted to an upvote rate of exactly 0.68. Plugging in the values makes this apparent: \\[ \\frac{10 \\cdot 0.68 + upvotes}{10 + upvotes + downvotes} = \\frac{68 + upvotes}{100 + upvotes + downvotes} \\] Graphically, it looks like this (the light grey line is the plain average for comparison): As you can see, the cumulative Bayesian average is much less erratic when little data is available. However, there is an important question left: How do we chose a good prior and a good weight? Here is how the Bayesian average develops for different prior beliefs about the average. Bayesian averages over time are indicated by light grey lines, the priors chosen here are 0.1 through 0.9 and the weight is kept constant at 20. The plain average is overlayed in darker grey for reference. And here is that same plot with the same priors, but with a weight of 100. TODO: write a section about how to chose a prior "],["cognitive-dissonance-and-information-gain.html", "Chapter 4 Cognitive Dissonance and Information Gain 4.1 Key Concepts from Information Theory 4.2 Surprisal as a Measure of Error 4.3 Total Cross Entropy 4.4 Total Relative Entropy = Cognitive Dissonance 4.5 Information Gain 4.6 Discussion", " Chapter 4 Cognitive Dissonance and Information Gain As described in the Global Brain Overview, the goal of the global brain algorithm is to focus users‚Äô attention on posts that reduce cognitive dissonance ‚Äì difference of belief that exist only exist because people have been exposed to different information. When a note on a post changes the probability that users upvote the post, then there is cognitive dissonance in proportion to the number of people who voted on the post without being shown the note. Information theory lets us easily quantify this cognitive dissonance using the concept of entropy. In information theory, information is simply the reduction of entropy. So reducing cognitive dissonance can also be seen as gaining information. So the goal of the global brain algorithm should be to direct attention to notes with the greatest potential information gain. 4.1 Key Concepts from Information Theory Here‚Äôs a quick summary of some basic concepts of information theory, building up from simple to complex concepts. surprisal: how surprised I am when I learn that the value of X is x: \\[Suprisal(x) = -lg~P(X=x)\\] entropy: how surprised I expect to be: \\[ \\begin{aligned} H(P) &amp;= ùîº Suprisal(X) \\\\ &amp;= ùîº -lg~P(X) \\\\ &amp;= ‚àë_x P(X=x) √ó -lg~P(X=x) \\\\ \\end{aligned} \\] cross-entropy: how surprised I expect Bob to be (if Bob‚Äôs beliefs are \\(Q\\) instead of \\(P\\)): \\[ \\begin{aligned} H(P,Q) &amp;= ùîº Suprisal_Q(X) \\\\ &amp;= ùîº -lg~Q(X) \\\\ &amp;= ‚àë_x P(X=x) √ó -lg~Q(X=x) \\end{aligned} \\] relative entropy or KL divergence: how much more surprised I expect Bob to be than me: \\[ \\begin{aligned} Dkl(P || Q) &amp;= H(P,Q) - H(P) \\\\ &amp;= ‚àë_x P(X=x) √ó lg~\\frac{P(X=x)}{Q(X=x)} \\end{aligned} \\] When dealing with binary variables then these formulas can be written as: entropy: \\[ H(p) = - p √ó lg~p - (1-p) √ó lg~(1-p) \\] cross-entropy: \\[ H(p,q) = -p √ó -lg~q - (1-p) √ó lg~(1-q) \\] relative entropy or KL-divergence: \\[ Dkl(p||q) = - p √ó lg~\\frac{p}{q} - (1-p) √ó lg~\\frac{1-p}{1-q} \\] 4.2 Surprisal as a Measure of Error Surprisal can be thought of, for our purposes, as a measure of the ‚Äúerror‚Äù of a prediction. Whenever we predict something is very unlikely to happen, and it happens, then there is a large amount of surprisal, or error. If we predicted it was likely to happen, and it happens, there is only a small amount of error. For example, suppose we predict that the probability of a user upvoting a post is \\(p\\). Then suppose there are actually \\(upvotes\\) upvotes and \\(downvotes\\) downvotes. What is the total error of our predictions? Every time there is an upvote, surprisal is \\(-lg(p)\\). Our estimate of the probability of a downvote is \\(1-p\\), so whenever there is a downvote surprisal is \\(-lg(1-p)\\). So our total error is: \\[ upvotes √ó -lg(p) + downvotes √ó -lg(1-p) \\] Since \\(upvotes ‚âà votesTotal√óp\\), and \\(downvotes ‚âà votesTotal√ó(1-p)\\) our total error is approximately: \\[ \\begin{aligned} &amp; votesTotal √ó p √ó -lg(p) + votesTotal √ó (1-p) √ó -lg(1-p) \\\\ &amp; = votesTotal √ó H(p) \\end{aligned} \\] In other words, our error is roughly entropy per vote, times the number of votes. 4.3 Total Cross Entropy Let‚Äôs say Alice estimates the probability of an upvote to be \\(q\\), but Bob thinks the probability of an upvote is \\(p\\). So Bob‚Äôs measure of Alice‚Äôs error will different from Alice‚Äôs expectation of her own error! Bob‚Äôs measure of Alice‚Äôs error is: \\[ \\begin{aligned} &amp;~~upvotes √ó -lg(q) + downvotes √ó -lg(1-q) \\\\ &amp;= votesTotal√óp √ó -lg(q) + votesTotal√ó(1-p) √ó -lg(1-q) ) \\\\ &amp;= votesTotal√ó( p√ó-lg(q) + (1-p)√ó-lg(1-q) ) \\\\ &amp;= votesTotal√óH(p,q) \\\\ \\end{aligned} \\] \\(H(p,q)\\) is the cross entropy between Bob and Alices‚Äôs probabilities: Bob‚Äôs estimate of how surprised Alice will be on average. In our case, Alice represents the average uninformed user (users who haven‚Äôt seen the note on a post), and Bob represents the informed user. There are \\(votesTotal\\) ‚ÄúAlices‚Äù, or uninformed users. Bob expects the total error of all the uninformed users to be \\(votesTotal√óH(p,q)\\). 4.4 Total Relative Entropy = Cognitive Dissonance The difference between \\(H(p,q)\\) and \\(H(p)\\) is the relative entropy, also known as the Kullback‚ÄìLeibler divergence or KL-divergence. It can be thought of as ‚Äúhow much more surprised I expect Alice to be than Bob‚Äù. A very interesting property of relative entropy is that it is never negative. It doesn‚Äôt matter whether \\(p\\) is greater or less than \\(q\\). Plug in different values to the formula to see. So Bob measure of Alice‚Äôs error is always greater than his measure of his own error as long as \\(p ‚â† q\\). He always expect Alice to be more surprised than him. That is because presumably Bob thinks he knows something Alice doesn‚Äôt know. If Bob thought Alices‚Äôs estimate \\(q\\) was better than his estimate \\(p\\), Bob would change his estimate to \\(q\\)! Relative entropy is zero if \\(p\\) and \\(q\\) are the same, because cross entropy \\(H(p,q)\\) just equals entropy \\(H(p)\\) when \\(p=q\\) (because \\(H(p,p)=H(p)\\)). If we take Bob‚Äôs measure of Alices‚Äôs total error, minus his measure of his own total error, we get the total relative entropy: how much more error Bob expects for Alice than he would expect if Alice knew more. This total error is our measure of cognitive dissonance. \\[ \\begin{aligned} cognitiveDissonance &amp;= votesTotal √ó H(p,q) - votesTotal √ó H(p) \\\\ &amp;= votesTotal √ó ( H(p,q) - H(p) ) \\\\ &amp;= votesTotal √ó Dkl(p || q) \\end{aligned} \\] So whenever the global brain learns that a note has change the probability of an upvote on a post, \\(p\\) becomes different from \\(q\\) and therefore cognitive dissonance is created. But as users are shown the note and change their votes so that \\(q\\) once again approaches \\(p\\), cognitive dissonance is reduced. 4.5 Information Gain When users change their votes after seeing a note, \\(q\\) approaches \\(p\\) and relative entropy is reduced. The reduction in relative-entropy is known as the information gain. The information gain from moving from \\(q\\) to \\(q_1\\), given informed probability \\(p\\), is \\[ \\begin{aligned} IG_p(q, q1) &amp;= Dkl(p || q) - Dkl(p || q_1) \\\\ &amp;= p √ó lg~(q1/q) + (1-p) √ó lg~((1-q1)/(1-q)) \\end{aligned} \\] The total information gain is: \\[ \\begin{aligned} totalInformationGain &amp;= votesTotal * IG_p(q, q1) \\\\ &amp;= votesTotal √ó ( Dkl(p || q) - Dkl(p || q_1) ) \\\\ &amp;= cognitiveDissonance - cognitiveDissonance\\_1 \\end{aligned} \\] 4.5.1 Example Suppose a post without a note is given receives 900 upvotes and 100 downvotes. But a certain note, when shown along with the post, reduces the upvote probability to 20%, without changing the vote rate. The total cognitive dissonance is: \\[ \\begin{aligned} cognitiveDissonance &amp;= votesTotal √ó Dkl(p || q) \\\\ &amp;= votesTotal √ó ( p √ó lg~\\frac{p}{q} + (1-p) √ó lg~\\frac{1-p}{1-q} ) \\\\ &amp;= 1000 √ó ( .2 √ó lg~\\frac{.2}{.9} + .8 √ó lg~\\frac{.8}{.1} ) \\\\ &amp;= 1966.01 \\end{aligned} \\] Suppose that, after showing the note to users that already upvoted the post, 70 users change their upvote to a downvote. The new upvote probability \\(q_1\\) will be approximately \\[ \\begin{aligned} q_1 &amp;‚âà \\frac{upvotes}{votesTotal} \\\\ &amp;= \\frac{votesTotal√óq - 70}{votesTotal} \\\\ &amp;= \\frac{1000 * .90 - 70}{1000} \\\\ &amp;= .83 \\end{aligned} \\] The new cognitive dissonance will therefore be \\[ \\begin{aligned} cognitiveDissonance_1 \\\\ &amp;= votesTotal √ó DKL(p, q_1)\\\\ &amp;= votesTotal √ó DKL(.2, .83 ) \\\\ &amp;= 1376.95~bits \\end{aligned} \\] The reduction in cognitive dissonance is the total information gain: \\[ \\begin{aligned} totalInformationGain &amp;= cognitiveDissonance - cognitiveDissonance_1 \\\\ &amp;= 1966.01~bits - 1376.95~bits \\\\ &amp;= 589.07~bits \\end{aligned} \\] Which can also be calculated as: \\[ \\begin{aligned} totalInformationGain &amp;= votesTotal √ó IG_p(q, q1) \\\\ &amp;= votesTotal √ó p √ó lg~(q1/q) + (1-p) √ó lg~((1-q1)/(1-q)) \\\\ &amp;= 1000 √ó 0.58907~bits \\\\ &amp;= 589.07~bits \\end{aligned} \\] The table below shows how relative entropy falls as users change upvotes to downvotes, reaching zero when \\(p=q_t\\) and therefore \\(upvotes=p√óvotesTotal=200\\). q_t upvotes dkl dissonance informationGain 0.90 900 1.9660150 1966.01500 NA 0.83 830 1.3769499 1376.94994 589.06506 0.76 760 1.0043726 1004.37259 372.57734 0.69 690 0.7368662 736.86616 267.50644 0.62 620 0.5327468 532.74682 204.11933 0.55 550 0.3721737 372.17368 160.57315 0.48 480 0.2445838 244.58382 127.58985 0.41 410 0.1443033 144.30325 100.28057 0.34 340 0.0689202 68.92023 75.38302 0.27 270 0.0190909 19.09095 49.82928 0.20 200 0.0000000 0.00000 19.09095 Note how changing upvotes produces diminishing returns. Focusing attention on posts and notes that maximize the rate of information gain is the overall goal of the global brain algorithm. See the next document on [information-rate.html]. 4.6 Discussion 4.6.1 Parallel to Machine Learning Cross entropy is commonly used used as the cost function in many machine learning algorithms. A neural network for example takes an input with labels (e.g.¬†images of cats and dogs) and outputs a probability (e.g.¬†that the image is a cat). The cost function computes how far these probability estimates are from the correct labels, and the neural network is trained by minimizing the cost function. If \\(yÃÇ_i\\) is the machine‚Äôs predicted probability for training example \\(i\\), and \\(y_i\\) is the correct output (1 or 0), then the total cross entropy cost is: \\[ \\sum_i H(y_i, yÃÇ) = \\sum_i y_i √ó -lg(yÃÇ_i) + (1-y_i) √ó -lg(1 - yÃÇ_i) \\] In our case, if we say that \\(y_i\\) are users votes, and \\(yÃÇ_i\\) is always equal to the uninformed users prediction \\(q\\), then our cost function is identical to the cost function used when training a neural network: \\[ \\begin{aligned} \\sum_i H(y_i, q) &amp;= \\sum_i y_i √ó -lg(yÃÇ_i) + (1-y_i) √ó -lg(1 - yÃÇ_i) \\\\ &amp;= \\sum_i y_i √ó -lg(q) + (1-y_i) √ó -lg(1 - q) \\\\ &amp;= upvotes √ó -lg(q) + downvotes √ó -lg(1 - q) \\\\ &amp;‚âà votesTotal√óH(p,q) \\\\ \\end{aligned} \\] So both neural networks and the global brain ‚Äúlearn‚Äù by reducing cross entropy. The difference is that the global brain reduces entropy not by learning to make better predictions, but by in a sense teaching users to make better predictions of how a fully-informed user would vote. 4.6.2 A Subtle Point Note that cross-entropy in our case is a measure of the total surprise of uninformed users at the hypothetical future votes of informed users. That is to say, it is: \\[ \\begin{aligned} &amp; votesTotal√óp √ó -lg(q) + votesTotal√ó(1-p) √ó -lg(1-q) ) \\\\ &amp;‚âà hypotheticalInformedUpvotes √ó -lg(q) + hypotheticalInformedDownvotes √ó -lg(1-q) ) \\\\ &amp;= votesTotal√óH(p,q) \\end{aligned} \\] And not \\[ \\begin{aligned} &amp; votesTotal√óq √ó -lg(q) + votesTotal√ó(1-q) √ó -lg(1-q) ) \\\\ &amp;‚âà actualUpvotes √ó -lg(q) + actualDownvotes √ó -lg(1-q) ) \\\\ &amp;= votesTotal√óH(q) \\end{aligned} \\] This is potentially confusing (it caused me a great deal of confusion before I figured this out) because we are measuring the error of Alice‚Äôs estimated probability \\(q\\) with respect to hypothetical events that have have not yet actually occurred. But shouldn‚Äôt we be measuring the error of the votes that have occurred? No, because surprisal is a measure of the error of a probability estimate: how likely the event was estimated to be, vs.¬†what event actually occurred. The uninformed user‚Äôs total surprisal about uninformed vote events is just \\(votesTotal*H(q)\\). But this tells us nothing about error with respect to an informed users‚Äôs beliefs. The informed users‚Äôs total surprisal about uninformed votes \\(votesTotal*H(q,p)\\) is potentially interesting is a measure of the distance between actual and informed belief, but is less correct because it is measuring Bob‚Äôs error, not Alices‚Äôs. \\(votesTotal√óH(p,q)\\) makes the most sense as a measure of the tension between the current state of user‚Äôs beliefs, and what that state should be, which is a hypothetical future where all users have changed their upvotes because of the information in the note, and thus \\(actualUpvotes = hypotheticalInformedUpvotes\\) and \\(actualDownvotes = hypotheticalInformedDownvotes\\). "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

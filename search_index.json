[["index.html", "A Minimal Book Example Chapter 1 About", " A Minimal Book Example Last updated: 2023-10-12 Chapter 1 About This document contains write-ups and explanations on the Y platform, currently created by the social protocols organization. At this point, Y has not been launched yet and is under rapid development, so this is a living document and the information in it may become outdated fast. Nonetheless, we try to maintain up-to-date explanations of our ideas here. We will introduce the building blocks of our algorithms; simple, but important concepts which make up the foundation of the components from which the Y platform is built. We are grateful for feedback. Do not hesitate to send a mail. TODO: describe how Y works in broad terms describe ratable unit of information "],["modeling-upvote-rates.html", "Chapter 2 Modeling Upvote Rates 2.1 Modeling Belief about the “True” Upvote Rate 2.2 Estimating the True Upvote Rate 2.3 The Bayesian Average", " Chapter 2 Modeling Upvote Rates The two kinds of rating users can leave for posts are upvotes and downvotes. A naive way of using upvotes and downotes for rating a post would be to simply calculate the ratio between them: \\(upvotes:downvotes\\). Or expressed as the fraction of upvotes over all votes: \\[ \\frac{upvotes}{upvotes + downvotes} \\] We call this rate the upvote rate. Another way of thinking about the upvote rate is as an average vote. If we model the upvote rate as a random variable, we might encode upvotes with a value of \\(1\\) and downvotes with a value of \\(0\\). A sample post’s gathered votes at a certain point in time \\(t\\) might look like this: \\[ (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) \\] The post has gathered a total of 12 votes of which 3 are upvotes and 9 are downvotes. The upvote rate (i.e., the “average” vote) for this sample is: \\[ \\rho = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{upvotes}{upvotes + downvotes} = \\frac{3}{3 + 9} = 0.25 \\] 2.1 Modeling Belief about the “True” Upvote Rate We assume that each post has a “true” upvote rate that we can only estimate by collecting data on it (i.e., collecting votes on a post). We model the true upvote rate as a random variable that follows a Beta distribution: \\[ upvoteRate \\sim \\beta(upvotes, downvotes) \\] The Beta distribution is a suitable model for proportions or probabilities. Yet another way of thinking about the upvote rate is as the probability of a vote being an upvote. The Beta distribution has two shape parameters \\(\\alpha\\) and \\(\\beta\\) which in our case are given by our upvote and downvote counts. To provide a more intuitive understanding of how this distribution models our upvote rate, here is an example of how it develops for an example post. We initialize the distribution with a prior of 1 upvote and 1 downvote. This is equivalent to a uniform distribution: We assign equal probability to any outcome, i.e., to any upvote rate. Now let’s use the previous example vote sample from above: \\[ (1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0) \\] Using the example above, we get the following distributions at different points in time: As you can see, the distribution updates and the probability mass becomes more concentrated around fewer values which means we gain make be more and more certain about our beliefs about the true upvote rate. If the post above would develop further with a similar trajectory, the following could be the outcome after 500 votes: We are now relatively certain that the true upvote rate is somewhere between around 0.2 and 0.3. 2.2 Estimating the True Upvote Rate Now that we can express our beliefs about the true upvote rate, how do we make a “best guess” at any given point in time? A naive solution would be to just take the sample upvote rate as introduced above: \\[ P(U) = \\frac{\\sum_{i=1}^n x_i}{n} = \\frac{upvotes}{upvotes + downvotes} \\] This solution is naive because it ignores an important fact: In the beginning, we have very little information (in fact no information) about the upvote rate of the post. Thus, each new arriving vote has an outsized effect on our estimate that just gets increasingly small over time. What this would result in are very erratic estimates in the beginning which would only smooth out over time. Here is how the cumulative mean develops for a random vote history with a true upvote rate of 0.6: If we were to use this metric to compare posts, getting a very high estimate would essentially come down to luck in the early stages of a post. It might then temporarily fare overly well (or overly poorly) compared to other posts. We have to take into account prior information and context to avoid this. 2.3 The Bayesian Average The Bayesian Average uses a weighted prior estimate of the average to avoid the erratic shifts in the estimate when there is not a lot of data. It is calculated as follows: \\[ \\frac{Cm + \\sum_{i=1}^n x_i}{C + n} \\] \\(C\\) is a weight constant and \\(m\\) is our prior belief about the average. But what does this achieve in concrete terms? Let’s build up to this formula step by step using our example of estimating the true upvote rate of a post. First, remember that the sample upvote rate constitutes the “plain” average. Let’s say our dataset \\(X\\) consists of \\(1\\)s and \\(0\\)s, where \\(1\\) denotes an upvote and \\(0\\) denotes a downvote. Then \\(\\sum_{i=1}^n{x_i}\\) is the number of upvotes (because downvotes are encoded as \\(0\\)s) and \\(n\\) is the total number of votes \\(upvotes + downvotes\\). The plain average is calculated as follows: \\[ \\frac{\\sum_{i=1}^n x_i}{n} \\] Or using our use case: \\[ \\frac{upvotes}{upvotes + downvotes} \\] The Bayesian average is thus: \\[ \\frac{C \\cdot m + upvotes}{C + upvotes + downvotes} \\] Now what does it mean that we add \\(C \\cdot m\\) to the nominator and \\(C\\) to the denominator? Practically, adding these terms to our formula means that we calculate the cumulative average as if we had collected \\(C\\) votes with an upvote rate of \\(m\\) before we collected the first vote on our post. If our prior belief about the average is 0.68 and we chose a weighting factor of 10, this would mean that we calculate the average as if we had previously collected 10 data points which amounted to an upvote rate of exactly 0.6. Plugging in the values makes this apparent: \\[ \\frac{10 \\cdot 0.68 + upvotes}{10 + upvotes + downvotes} = \\frac{68 + upvotes}{100 + upvotes + downvotes} \\] Graphically, it looks like this (the light grey line is the plain average for comparison): As you can see, the cumulative Bayesian average is much less erratic when little data is available. However, there is an important question left: How do we chose a good prior and a good weight? Here is how the Bayesian average develops for different prior beliefs about the average (indicated by light grey lines, the priors chosen here are 0.1 through 0.9 and the weight is kept constant at 20): And here is how this would look like with a weight of 100: How to properly seed the Bayesian average is still an open question. For now, we will chose a “common sense” global prior that will be used for all posts. "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]

# A Primer on Information Theory II

In machine learning, cross-entropy loss is common not because it is in any way different from KL divergence loss, but simply because it has a simpler formula.
The two losses are actually equivalent.



## Cross-Entropy

"How surprised I expect Bob to be" (Jonathan)

Wikipedia:
"In information theory, the cross-entropy between two probability distributions $p$ and $q$ over the same underlying set of events measures the average number of bits needed to identify an event drawn from the set if a coding scheme used for the set is optimized for an estimated probability distribution $q$, rather than the true distribution $p$."

$$
H(p, q) = -\sum_{x \in X}p(x)log(q(x))
$$






## KL Divergence

KL divergence is a measure of distance between two probability distributions.

RATIONALE TAKEN FROM: https://www.youtube.com/watch?v=SxGYPqCgJWM

Main question:
Do two probability distributions assign similar probability to a sequence of events?

Let's say we have two distributions, P and Q.
For a coin toss, we can calculate the probability of a sequence of tosses for each distribution.

$$
P(x) = p_1^{N_H}p_2^{N_T} \\
Q(x) = q_1^{N_H}q_2^{N_T}
$$

Then, we take the ratio between the two probabilities:

$$
\frac{P(x)}{Q(x)} = \frac{p_1^{N_H}p_2^{N_T}}{q_1^{N_H}q_2^{N_T}}
$$

Even though this is not the final formula for the KL divergence, it *does* calculate something along those lines: the ratio between the probabilities.

We then <span style="color: blue">normalize this measure by the sample size</span> and <span style="color: firebrick">take the logarithm</span>:

$$
\color{firebrick}{log} (\frac{p_1^{N_H}p_2^{N_T}}{q_1^{N_H}q_2^{N_T}})^\color{blue}{\frac{1}{N}}
$$

From video (see above):
"How likely would the second distribution be to generate samples from the first?"


